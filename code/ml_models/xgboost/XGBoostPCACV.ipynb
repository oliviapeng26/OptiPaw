{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4074,"status":"ok","timestamp":1728523943367,"user":{"displayName":"Ng YC","userId":"11737261317625661208"},"user_tz":-660},"id":"vBxUkBy2SMnx","outputId":"6bc87f78-e139-4a97-d90b-f9c8844bdadb"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Animal.ID    Name Animal.Type                        Breed            Sex  \\\n","0   A006100   Scamp         Dog         Spinone Italiano Mix  Neutered Male   \n","1   A006100   Scamp         Dog         Spinone Italiano Mix  Neutered Male   \n","2   A047759    Oreo         Dog                    Dachshund  Neutered Male   \n","3   A134067  Bandit         Dog            Shetland Sheepdog  Neutered Male   \n","4   A141142  Bettie         Dog  Labrador Retriever/Pit Bull  Spayed Female   \n","\n","          Color   Age      Intake.Type     Outcome.Type          Intake.Date  \\\n","0  Yellow/White   7.0    Public Assist  Return to Owner  2014-12-19 10:21:00   \n","1  Yellow/White   6.0    Public Assist  Return to Owner  2014-03-07 14:26:00   \n","2      Tricolor  10.0  Owner Surrender         Transfer  2014-04-02 15:55:00   \n","3   Brown/White  16.0    Public Assist  Return to Owner  2013-11-16 09:02:00   \n","4   Black/White  15.0            Stray  Return to Owner  2013-11-16 14:46:00   \n","\n","          Outcome.Date   State  \n","0  2014-12-20 16:35:00  Austin  \n","1  2014-03-08 17:10:00  Austin  \n","2  2014-04-07 15:12:00  Austin  \n","3  2013-11-16 11:54:00  Austin  \n","4  2013-11-17 11:40:00  Austin  \n"]}],"source":["# Remember to upload optipaw_FINAL.csv\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Set options to display all unique values\n","# pd.set_option('display.max_rows', None)  # Display all rows\n","# pd.set_option('display.max_columns', None)  # Display all columns\n","\n","# Load Optipaw Data\n","optipaw_data = pd.read_csv('optipaw_FINAL.csv')\n","\n","# Extract rows where the 'State' column is 'Austin'\n","austin_data = optipaw_data[optipaw_data['State'] == 'Austin'].copy()\n","\n","# Reset the index for the Austin data (optional but useful for clean DataFrames)\n","austin_data.reset_index(drop=True, inplace=True)\n","\n","# Display the first few rows of the extracted data\n","print(austin_data.head())"]},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":true,"executionInfo":{"elapsed":38565,"status":"ok","timestamp":1728523981927,"user":{"displayName":"Ng YC","userId":"11737261317625661208"},"user_tz":-660},"id":"iutf7TsHTqtK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7b1ee7a5-4860-40a8-be02-c6ca688ba1c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Animal.ID   Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type   State  \\\n","0   A006100  19560            1    1   7.0            1             1  Austin   \n","1   A006100  19560            1    1   6.0            1             1  Austin   \n","2   A047759  16845            1    1  10.0            2             2  Austin   \n","3   A134067   4665            1    1  16.0            1             1  Austin   \n","4   A141142   5095            1    2  15.0            3             1  Austin   \n","\n","   Abyssinian  Affenpinscher  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0           0              0  ...          19            12         2014   \n","1           0              0  ...           7             3         2014   \n","2           0              0  ...           2             4         2014   \n","3           0              0  ...          16            11         2013   \n","4           0              0  ...          16            11         2013   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           20             12          2014           10            16   \n","1            8              3          2014           14            17   \n","2            7              4          2014           15            15   \n","3           16             11          2013            9            11   \n","4           17             11          2013           14            11   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.617994              4.188790  \n","1             3.665191              4.450590  \n","2             3.926991              3.926991  \n","3             2.356194              2.879793  \n","4             3.665191              2.879793  \n","\n","[5 rows x 436 columns]\n"]}],"source":["# Preprocessing Function for ML\n","def preprocessing(df, name_mapping_var=None):\n","\n","  # For Name, we will use label encoding to assign each unique name a specific int, at the same time return the mapping\n","  # Strip leading asterisks from the Name column\n","  df['Name'] = df['Name'].str.lstrip('*')\n","  label_encoder = LabelEncoder()\n","  df['Name'] = label_encoder.fit_transform(df['Name'].astype(str))\n","\n","  # If the user provided a variable to store the mapping, assign it\n","  if name_mapping_var is not None:\n","      name_mapping = {k: v for v, k in enumerate(label_encoder.classes_)}\n","      name_mapping_var.update(name_mapping)\n","\n","\n","  # For Animal.Type we will map Int Values to the specific animal type\n","  animal_mapping = {\n","    \"Dog\": 1, \"Cat\": 2, \"Other\": 3, \"Bird\": 4, \"Livestock\": 5,\n","    \"House Rabbit\": 6, \"Rat\": 7, \"Ferret\": 8, \"Pig\": 9, \"Hamster\": 10,\n","    \"Guinea Pig\": 11, \"Gerbil\": 12, \"Hedgehog\": 13, \"Chinchilla\": 14,\n","    \"Goat\": 15, \"Mouse\": 16, \"Sugar Glider\": 17, \"Snake\": 18,\n","    \"Wildlife\": 19, \"Lizard\": 20\n","    }\n","\n","  df['Animal.Type'] = df['Animal.Type'].map(animal_mapping)\n","\n","  # For Breed we will perform one hot encoding onto it\n","  # Remove parentheses and their contents, and replace '/' with space\n","  df['Breed'] = df['Breed'].str.replace(r'\\(.*?\\)', '', regex=True).str.replace('/', ' ').str.replace(',', ' ')\n","\n","  # Split the 'Breed' column into a list and capitalize the first letter of each word\n","  df['Breed'] = df['Breed'].str.split().apply(lambda breeds: [breed.rstrip('-').capitalize() for breed in breeds])\n","\n","  # Create dummy variables for each unique breed\n","  df_breeds = df['Breed'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded breed dataframe\n","  df = pd.concat([df, df_breeds], axis=1)\n","\n","  # Drop the original 'Breed' column\n","  df = df.drop(columns=['Breed'])\n","\n","  # For Sex we will map Int Values to specific Sex\n","  sex_mapping = {'Neutered Male': 1, 'Spayed Female': 2, 'Intact Female': 3, 'Intact Male': 4, 'Unknown': 5, 'Female': 6, 'Male': 7}\n","\n","  # Map the Sex column using the defined mapping\n","  df['Sex'] = df['Sex'].map(sex_mapping)\n","\n","  # For colours, we will split into individual colours and use one hot encoding, which is assigning binary values to it\n","\n","  # Split the 'Color' column by '/', 'and', and ','\n","  df['Color'] = df['Color'].str.replace('/', ' ').str.replace('and', ' ').str.replace(',', ' ').str.replace(r'-\\b', '', regex=True)\n","\n","  # Split the 'Color' column into a list and capitalize the first letter of each word\n","  df['Color'] = df['Color'].str.split().apply(lambda colors: [color.capitalize() for color in colors])\n","\n","  # Create dummy variables for each unique color\n","  df_colors = df['Color'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded color dataframe\n","  df = pd.concat([df, df_colors], axis=1)\n","\n","  # Drop the original 'Color' column\n","  df = df.drop(columns=['Color'])\n","\n","  # For Age, we will just store it as int and impute it with 0 if it is null, and store it as float\n","  df['Age'] = df['Age'].fillna(0).astype(float)\n","\n","  # For Intake.Type, we will map Int Values to specific Intake\n","  intake_type_mapping = {\n","      'Public Assist': 1, 'Owner Surrender': 2, 'Stray': 3, 'Euthanasia Request': 4,\n","      'Abandoned': 5, 'Wildlife': 6, 'Moving': 7, 'Incompatible with owner lifestyle': 8,\n","      'Rabies Monitoring': 9, 'Marriage/Relationship split': 10, 'Owner Deceased': 11, 'Police Assist': 12,\n","      'Biting': 13, 'Owner Died': 14, 'TNR - Trap/Neuter/Release': 15, 'Unable to Afford': 16,\n","      'Unsuitable Accommodation': 17, 'Allergies': 18, 'Transfer from Other Shelter': 19,\n","      'Born in Shelter': 20, 'Landlord issues': 21, 'Litter relinquishment': 22, 'Sick/Injured': 23,\n","      'Owner requested Euthanasia': 24, 'Abuse/ neglect': 25, 'Incompatible with other pets': 26,\n","      'Behavioral Issues': 27, 'DOA': 28\n","  }\n","\n","  # Map the Intake.Type column using the defined mapping\n","  df['Intake.Type'] = df['Intake.Type'].map(intake_type_mapping)\n","\n","  # For Outcome.Type, we will map Int Values to specific Outcome\n","  outcome_type_mapping = {\n","      'Return to Owner': 1, 'Transfer': 2, 'Adoption': 3, 'Euthanasia': 4,\n","      'Died': 5, 'Rto-Adopt': 6, 'Disposal': 7, 'Missing': 8,\n","      'Stolen': 9, 'Relocate': 10, 'Lost': 11, 'Foster': 12,\n","      'Reclaimed': 13, 'Escaped': 14, 'Released To Wild': 15\n","  }\n","\n","  # Map the Outcome.Type column using the defined mapping\n","  df['Outcome.Type'] = df['Outcome.Type'].map(outcome_type_mapping)\n","\n","  # For Date and Time, we will be using panda and numpy date conversion\n","\n","  # Convert Intake.Date and Outcome.Date to datetime format\n","  df['Intake.Date'] = pd.to_datetime(df['Intake.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","  df['Outcome.Date'] = pd.to_datetime(df['Outcome.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","\n","  # Extract date components from the date columns\n","  df['Intake.Day'] = df['Intake.Date'].dt.day.fillna(0).astype(int)\n","  df['Intake.Month'] = df['Intake.Date'].dt.month.fillna(0).astype(int)\n","  df['Intake.Year'] = df['Intake.Date'].dt.year.fillna(0).astype(int)\n","\n","  df['Outcome.Day'] = df['Outcome.Date'].dt.day.fillna(0).astype(int)\n","  df['Outcome.Month'] = df['Outcome.Date'].dt.month.fillna(0).astype(int)\n","  df['Outcome.Year'] = df['Outcome.Date'].dt.year.fillna(0).astype(int)\n","\n","  # Extract and convert the hour to radians\n","  df['Intake.Hour'] = df['Intake.Date'].dt.hour.fillna(0).astype(int)\n","  df['Outcome.Hour'] = df['Outcome.Date'].dt.hour.fillna(0).astype(int)\n","\n","  df['Intake.Hour.Radians'] = (df['Intake.Hour'] / 24) * 2 * np.pi\n","  df['Outcome.Hour.Radians'] = (df['Outcome.Hour'] / 24) * 2 * np.pi\n","\n","  # Drop original date columns if no longer needed\n","  df = df.drop(columns=['Intake.Date', 'Outcome.Date'])\n","\n","  return df\n","\n","# Process Austin Dataset (Animal ID and State to be removed later)\n","austin_data = preprocessing(austin_data)\n","\n","print(austin_data.head())\n"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1140,"status":"ok","timestamp":1728523983062,"user":{"displayName":"Ng YC","userId":"11737261317625661208"},"user_tz":-660},"id":"GtsN2gkuUgnk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"80dc0946-218a-45df-e1c6-f4632034a4cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Animal.ID                object\n","Name                      int64\n","Animal.Type               int64\n","Sex                       int64\n","Age                     float64\n","                         ...   \n","Outcome.Year              int64\n","Intake.Hour               int64\n","Outcome.Hour              int64\n","Intake.Hour.Radians     float64\n","Outcome.Hour.Radians    float64\n","Length: 436, dtype: object\n","(115498, 436)\n","Animal.ID               99642\n","Name                    23514\n","Animal.Type                 5\n","Sex                         5\n","Age                        42\n","                        ...  \n","Outcome.Year               12\n","Intake.Hour                24\n","Outcome.Hour               24\n","Intake.Hour.Radians        24\n","Outcome.Hour.Radians       24\n","Length: 436, dtype: int64\n","Animal.ID               0\n","Name                    0\n","Animal.Type             0\n","Sex                     0\n","Age                     0\n","                       ..\n","Outcome.Year            0\n","Intake.Hour             0\n","Outcome.Hour            0\n","Intake.Hour.Radians     0\n","Outcome.Hour.Radians    0\n","Length: 436, dtype: int64\n"]}],"source":["# Unique value checks and Null value checks\n","# Print dtypes, unique and missing value checks before splitting\n","print(austin_data.dtypes)\n","print(austin_data.shape)\n","print(austin_data.nunique())\n","print(austin_data.isnull().sum())"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"executionInfo":{"elapsed":410,"status":"ok","timestamp":1728523983470,"user":{"displayName":"Ng YC","userId":"11737261317625661208"},"user_tz":-660},"id":"q1aUNY_PUusI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0743bb94-a52c-4360-8e4b-a3ccce60c5c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type  Abyssinian  \\\n","0  19560            1    1   7.0            1             1           0   \n","1  19560            1    1   6.0            1             1           0   \n","2  16845            1    1  10.0            2             2           0   \n","3   4665            1    1  16.0            1             1           0   \n","4   5095            1    2  15.0            3             1           0   \n","\n","   Affenpinscher  Afghan  African  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0              0       0        0  ...          19            12         2014   \n","1              0       0        0  ...           7             3         2014   \n","2              0       0        0  ...           2             4         2014   \n","3              0       0        0  ...          16            11         2013   \n","4              0       0        0  ...          16            11         2013   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           20             12          2014           10            16   \n","1            8              3          2014           14            17   \n","2            7              4          2014           15            15   \n","3           16             11          2013            9            11   \n","4           17             11          2013           14            11   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.617994              4.188790  \n","1             3.665191              4.450590  \n","2             3.926991              3.926991  \n","3             2.356194              2.879793  \n","4             3.665191              2.879793  \n","\n","[5 rows x 434 columns]\n","    Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type  Abyssinian  \\\n","0   9626            2    1  16.0            3             1           0   \n","1  20023            1    2  16.0            1             1           0   \n","2   4940            2    2  16.0            3             3           0   \n","3   9385            2    1  16.0            3             1           0   \n","4  15780            2    1  16.0            1             7           0   \n","\n","   Affenpinscher  Afghan  African  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0              0       0        0  ...          25             8         2023   \n","1              0       0        0  ...          27             4         2023   \n","2              0       0        0  ...          28             3         2023   \n","3              0       0        0  ...          23            10         2023   \n","4              0       0        0  ...          11             5         2024   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           25              8          2023           11            14   \n","1           28              4          2023           14            12   \n","2           17              4          2023           20            11   \n","3           24             10          2023           18            13   \n","4           26              5          2024           12             9   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.879793              3.665191  \n","1             3.665191              3.141593  \n","2             5.235988              2.879793  \n","3             4.712389              3.403392  \n","4             3.141593              2.356194  \n","\n","[5 rows x 434 columns]\n","0    A006100\n","1    A006100\n","2    A047759\n","3    A134067\n","4    A141142\n","Name: Animal.ID, dtype: object\n","0    A454956\n","1    A478575\n","2    A478962\n","3    A480389\n","4    A495162\n","Name: Animal.ID, dtype: object\n","(101615, 434)\n","(13883, 434)\n"]}],"source":["# Splitting austin_data based on Outcome.Year\n","train_data = austin_data[austin_data['Outcome.Year'] < 2023].copy()\n","test_data = austin_data[austin_data['Outcome.Year'] >= 2023].copy()\n","\n","# Reset the index for both datasets\n","train_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)\n","\n","# Drop 'State' column from both train and test datasets\n","train_data = train_data.drop(columns=['State'])\n","test_data = test_data.drop(columns=['State'])\n","\n","# Extract Animal.ID from the train_data and test_data\n","train_ids = train_data['Animal.ID']\n","test_ids = test_data['Animal.ID']\n","\n","# Drop 'Animal.ID' from train_data and test_data\n","train_data = train_data.drop(columns=['Animal.ID'])\n","test_data = test_data.drop(columns=['Animal.ID'])\n","\n","# Output the result to verify the split\n","print(train_data.head(5))\n","print(test_data.head(5))\n","\n","print(train_ids.head(5))\n","print(test_ids.head(5))\n","\n","print(train_data.shape)\n","print(test_data.shape)\n","\n","# Split dataset to x_train, y_train and x_test, y_test\n","x_train = train_data.drop(columns=['Outcome.Type'])\n","y_train = train_data['Outcome.Type']\n","train_id = train_ids\n","\n","x_test = test_data.drop(columns=['Outcome.Type'])\n","y_test = test_data['Outcome.Type']\n","test_id = test_ids\n","\n","\n"]},{"cell_type":"code","source":["# PCA Transformation\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import log_loss, classification_report\n","\n","# Step 2: Perform PCA on the training data only\n","pca = PCA(n_components=0.95)  # Retain 95% variance or set a fixed number of components\n","\n","# Fit PCA on the training data only\n","x_train_pca = pca.fit_transform(x_train)\n","\n","# Transform the test data using the PCA model fitted on the training data\n","x_test_pca = pca.transform(x_test)\n","\n","# Convert the result to a DataFrame for easier manipulation\n","x_train_pca = pd.DataFrame(x_train_pca)\n","x_test_pca = pd.DataFrame(x_train_pca)\n"],"metadata":{"id":"CdItLYS--4Wj","executionInfo":{"status":"ok","timestamp":1728523985028,"user_tz":-660,"elapsed":1561,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"OrDRvLXwYNeb","executionInfo":{"status":"ok","timestamp":1728523985028,"user_tz":-660,"elapsed":6,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"collapsed":true},"outputs":[],"source":["# Scoring Function\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","import numpy as np\n","import pandas as pd\n","\n","def scoring_function(y_pred_df, y_test_df, all_classes=None):\n","    '''\n","    Calculates various performance metrics for the predictions.\n","\n","    Parameters:\n","    y_pred_df: DataFrame containing predictions.\n","    y_test_df: DataFrame containing true labels.\n","    all_classes: List of all possible classes.\n","\n","    Returns:\n","    metrics: Dictionary containing accuracy, precision, recall, f1 score, and log loss.\n","    '''\n","\n","    # Merge the prediction and actual outcome dataframes using Animal ID\n","    df_combined = pd.merge(y_pred_df, y_test_df, on='Animal.ID')\n","\n","    if df_combined.empty:\n","        print(\"No data in the combined DataFrame.\")\n","        return None\n","\n","    # Convert the predicted probabilities into predicted classes (argmax)\n","    y_pred_class = df_combined.iloc[:, 1:-1].idxmax(axis=1).astype(int)\n","    y_true = df_combined['Outcome.Type'].astype(int)\n","\n","    # If all_classes is None, use unique classes from y_true\n","    if all_classes is None:\n","        all_classes = np.unique(y_true)  # Get unique classes from y_true\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_true, y_pred_class)\n","\n","    # Calculate metrics only for predicted classes\n","    unique_pred_classes = np.unique(y_pred_class)\n","    precision = precision_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    recall = recall_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    f1 = f1_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","\n","    # Ensure that y_pred_probs contains probabilities for all classes\n","    y_pred_probs = df_combined.iloc[:, 1:-1].values\n","\n","    # Create an array to ensure all classes are represented in predictions\n","    complete_probs = np.zeros((y_pred_probs.shape[0], len(all_classes)))  # Initialize with zeros\n","    for i, class_label in enumerate(all_classes):\n","        if class_label in df_combined.columns:\n","            complete_probs[:, i] = y_pred_probs[:, class_label - 1]  # Fill with actual probabilities\n","\n","    # Calculate log loss with all classes present in the labels\n","    logloss = log_loss(y_true, complete_probs, labels=all_classes)\n","\n","    # Print the metrics\n","    print(f\"\\nAccuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Log Loss: {logloss:.4f}\")\n","\n","    # Return the metrics as a dictionary\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,\n","        'log_loss': logloss\n","    }\n","\n","\n"]},{"cell_type":"code","source":["import warnings\n","\n","# Suppress FutureWarnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","from xgboost import XGBClassifier  # Import XGBClassifier\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","def perform_cross_validation(x_train, y_train, scoring_function, n_splits=5, n_repeats=10, save_path=\"xgboostPCACV_results.csv\"):\n","    \"\"\"\n","    Perform repeated cross-validation, collect the scoring metrics, and save results to a CSV.\n","\n","    Parameters:\n","    x_train: Training features\n","    y_train: Training labels (starting from 1)\n","    scoring_function: Function to compute the scoring metrics\n","    n_splits: Number of splits for K-fold cross-validation (default is 5)\n","    n_repeats: Number of times to repeat the cross-validation (default is 10)\n","    save_path: Path to save the CSV file (default is 'xgboostCV_results.csv')\n","\n","    Returns:\n","    results: A dictionary of lists where each list contains average values for the metrics (one per repetition)\n","    \"\"\"\n","    # Convert y_train to 0-based indexing\n","    y_train_adjusted = y_train - 1  # Adjust to 0-based indexing for internal processing\n","\n","    # Store metrics for all repetitions\n","    results = {\n","        'accuracy': [],\n","        'precision': [],\n","        'recall': [],\n","        'f1_score': [],\n","        'log_loss': []\n","    }\n","\n","    # DataFrame to hold all metrics across repetitions\n","    df_results = pd.DataFrame()\n","\n","    # Repeat the cross-validation process n_repeats times\n","    for repeat in range(n_repeats):\n","        print(f\"\\nCross-Validation Repeat: {repeat + 1}/{n_repeats}\")\n","\n","        # Use a different random_state for KFold in each repetition to introduce variability\n","        kf = KFold(n_splits=n_splits, shuffle=True, random_state=repeat)\n","\n","        # Store metrics for each fold within this repetition\n","        fold_metrics = {\n","            'accuracy': [],\n","            'precision': [],\n","            'recall': [],\n","            'f1_score': [],\n","            'log_loss': []\n","        }\n","\n","        # Perform K-fold cross-validation\n","        for train_idx, val_idx in kf.split(x_train):\n","            x_fold_train, x_fold_val = x_train.iloc[train_idx], x_train.iloc[val_idx]\n","            y_fold_train, y_fold_val = y_train_adjusted.iloc[train_idx], y_train_adjusted.iloc[val_idx]  # Use adjusted labels\n","\n","            try:\n","                # Train an XGBoost model\n","                xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n","                xgb_model.fit(x_fold_train, y_fold_train)\n","\n","                # Predict probabilities for validation set\n","                y_val_pred_probs = xgb_model.predict_proba(x_fold_val)\n","\n","                # Prepare the DataFrame for scoring\n","                num_classes = 15  # Assuming classes from 1 to 15\n","                predicted_df = pd.DataFrame(0, index=np.arange(y_val_pred_probs.shape[0]), columns=np.arange(1, num_classes + 1))\n","\n","                # Check if the number of classes predicted matches expectations\n","                for i in range(y_val_pred_probs.shape[1]):  # Iterate through the predicted probabilities\n","                    predicted_df[i + 1] = y_val_pred_probs[:, i]  # Fill in probabilities for classes 1 to 15\n","\n","                # Check for any missing classes\n","                if predicted_df.shape[1] < num_classes:\n","                    print(f\"Warning: {predicted_df.shape[1]} classes found, expected {num_classes}. Imputing missing classes with zeros.\")\n","\n","                # Prepare DataFrame for scoring\n","                predicted_df.insert(0, 'Animal.ID', val_idx)  # Simulate Animal IDs using validation indices\n","                y_test_df = pd.DataFrame({\n","                    'Animal.ID': val_idx,\n","                    'Outcome.Type': y_fold_val.reset_index(drop=True) + 1  # Convert back to 1-based for the scoring function\n","                })\n","\n","                # Use the scoring function\n","                metrics = scoring_function(predicted_df, y_test_df, all_classes=np.arange(1, num_classes + 1))\n","\n","                # Collect metrics for this fold\n","                for key in fold_metrics.keys():\n","                    fold_metrics[key].append(metrics[key])\n","\n","            except ValueError as e:\n","                if \"Invalid classes inferred from unique values of `y`\" in str(e):\n","                    print(f\"Skipping fold due to invalid classes: {e}\")\n","                    continue  # Skip this fold if there's a class mismatch\n","                else:\n","                    print(f\"Error in fold with indices {val_idx}: {e}\")\n","                    continue  # Handle other errors as needed\n","\n","        # Compute the average score for each metric across folds in this repetition\n","        avg_metrics = {key: np.mean(fold_metrics[key]) for key in fold_metrics.keys()}\n","\n","        # Print the average scores for this repetition\n","        print(f\"\\nAverage Metrics for Repetition {repeat + 1}/{n_repeats}:\")\n","        for key, value in avg_metrics.items():\n","            print(f\"{key.capitalize()}: {value:.4f}\")\n","\n","        # Append the average scores for each repetition to the results\n","        for key in results.keys():\n","            results[key].append(avg_metrics[key])\n","\n","        # Add results of this repetition to a DataFrame (for saving to CSV)\n","        avg_metrics['repetition'] = repeat + 1  # Add repetition number\n","        df_results = pd.concat([df_results, pd.DataFrame([avg_metrics])], ignore_index=True)\n","\n","    # Save the DataFrame to a CSV file after all repetitions\n","    df_results.to_csv(save_path, index=False)\n","    print(f\"\\nResults saved to {save_path}\")\n","\n","    return results\n","\n","# Function to plot the cross-validation results and save the plot as an image\n","def plot_cross_validation_results(results, save_path=\"XGBoostPCACV.png\"):\n","    \"\"\"\n","    Plot boxplots of the cross-validation results and save the plot as an image.\n","\n","    Parameters:\n","    results: A dictionary containing lists of metrics collected from cross-validation.\n","    save_path: Path to save the plot image (default is 'boxplot.png').\n","    \"\"\"\n","    # Convert results dictionary to DataFrame\n","    results_df = pd.DataFrame(results)\n","\n","    # Create the boxplot\n","    plt.figure(figsize=(10, 6))\n","    sns.boxplot(data=results_df)\n","    plt.title(\"XGBoost PCA 5-Fold Cross-Validation Metrics\")\n","    plt.ylabel(\"Score\")\n","\n","    # Save the plot as an image file\n","    plt.savefig(save_path, bbox_inches='tight')\n","\n","    # Display the plot\n","    plt.show()\n","\n","    print(f\"Boxplot saved to {save_path}\")\n","\n","\n"],"metadata":{"id":"mc5jWtxX4pRM","executionInfo":{"status":"ok","timestamp":1728523985403,"user_tz":-660,"elapsed":379,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Edit Here\n","\n","x_train_pca = x_train_pca.head(100)\n","y_train = y_train.head(100)\n","train_id = train_id.head(100)\n","\n","x_test_pca = x_test_pca.head(100)\n","y_test = y_test.head(100)\n","test_id = test_id.head(100)\n","\n","def make_unique_column_names(df):\n","    \"\"\"\n","    Makes the column names in the DataFrame unique by appending a suffix to duplicates.\n","\n","    Parameters:\n","    df (pd.DataFrame): Input DataFrame with potentially duplicate column names.\n","\n","    Returns:\n","    pd.DataFrame: DataFrame with unique column names.\n","    \"\"\"\n","    cols = pd.Series(df.columns)\n","    for dup in cols[cols.duplicated()].unique():  # Check for duplicates\n","        cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n","\n","    df.columns = cols  # Assign new unique column names\n","    return df\n","\n","x_train_pca = make_unique_column_names(x_train_pca)\n","x_test_pca = make_unique_column_names(x_test_pca)\n","\n","# Call the cross-validation function\n","results = perform_cross_validation(x_train_pca, y_train, scoring_function)\n","#print(results)\n","\n","# Plot the results\n","plot_cross_validation_results(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RYqZfzzm6BdN","executionInfo":{"status":"ok","timestamp":1728523992367,"user_tz":-660,"elapsed":6967,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"3f4e8593-61ce-4995-aa6d-28322fa3964a","collapsed":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Cross-Validation Repeat: 1/10\n","\n","Accuracy: 0.3500\n","Precision: 0.3889\n","Recall: 0.3500\n","F1 Score: 0.3571\n","Log Loss: 1.8822\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.3000\n","Precision: 0.3545\n","Recall: 0.3000\n","F1 Score: 0.3250\n","Log Loss: 2.1040\n","\n","Accuracy: 0.3500\n","Precision: 0.3833\n","Recall: 0.3500\n","F1 Score: 0.3625\n","Log Loss: 1.8669\n","\n","Accuracy: 0.2000\n","Precision: 0.2200\n","Recall: 0.2000\n","F1 Score: 0.2095\n","Log Loss: 3.0315\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.3929\n","Recall: 0.5714\n","F1 Score: 0.4656\n","Log Loss: 1.8479\n","\n","Average Metrics for Repetition 1/10:\n","Accuracy: 0.3200\n","Precision: 0.3479\n","Recall: 0.3543\n","F1_score: 0.3440\n","Log_loss: 2.1465\n","\n","Cross-Validation Repeat: 2/10\n","\n","Accuracy: 0.2500\n","Precision: 0.2101\n","Recall: 0.2941\n","F1 Score: 0.2451\n","Log Loss: 2.4704\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:05] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4500\n","Precision: 0.4700\n","Recall: 0.4500\n","F1 Score: 0.4306\n","Log Loss: 1.8328\n","\n","Accuracy: 0.3500\n","Precision: 0.4190\n","Recall: 0.4375\n","F1 Score: 0.4266\n","Log Loss: 2.4352\n","\n","Accuracy: 0.4000\n","Precision: 0.7111\n","Recall: 0.4000\n","F1 Score: 0.5120\n","Log Loss: 1.9807\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.3000\n","Precision: 0.2143\n","Recall: 0.3000\n","F1 Score: 0.2500\n","Log Loss: 2.5953\n","\n","Average Metrics for Repetition 2/10:\n","Accuracy: 0.3500\n","Precision: 0.4049\n","Recall: 0.3763\n","F1_score: 0.3728\n","Log_loss: 2.2629\n","\n","Cross-Validation Repeat: 3/10\n","\n","Accuracy: 0.4000\n","Precision: 0.4261\n","Recall: 0.4000\n","F1 Score: 0.3992\n","Log Loss: 2.1237\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.3200\n","Recall: 0.4000\n","F1 Score: 0.3556\n","Log Loss: 2.0314\n","\n","Accuracy: 0.3500\n","Precision: 0.4500\n","Recall: 0.3500\n","F1 Score: 0.3527\n","Log Loss: 2.4121\n","\n","Accuracy: 0.3500\n","Precision: 0.3401\n","Recall: 0.3684\n","F1 Score: 0.3537\n","Log Loss: 2.0037\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.2500\n","Precision: 0.3438\n","Recall: 0.2500\n","F1 Score: 0.2895\n","Log Loss: 2.3121\n","\n","Average Metrics for Repetition 3/10:\n","Accuracy: 0.3500\n","Precision: 0.3760\n","Recall: 0.3537\n","F1_score: 0.3501\n","Log_loss: 2.1766\n","\n","Cross-Validation Repeat: 4/10\n","\n","Accuracy: 0.3000\n","Precision: 0.3600\n","Recall: 0.3000\n","F1 Score: 0.3273\n","Log Loss: 2.2667\n","\n","Accuracy: 0.2500\n","Precision: 0.2143\n","Recall: 0.3333\n","F1 Score: 0.2609\n","Log Loss: 2.7159\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:06] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.4830\n","Recall: 0.4000\n","F1 Score: 0.4349\n","Log Loss: 2.0296\n","\n","Accuracy: 0.3500\n","Precision: 0.3250\n","Recall: 0.3500\n","F1 Score: 0.3370\n","Log Loss: 2.3097\n","\n","Accuracy: 0.4000\n","Precision: 0.3833\n","Recall: 0.4000\n","F1 Score: 0.3909\n","Log Loss: 1.9734\n","\n","Average Metrics for Repetition 4/10:\n","Accuracy: 0.3400\n","Precision: 0.3531\n","Recall: 0.3567\n","F1_score: 0.3502\n","Log_loss: 2.2591\n","\n","Cross-Validation Repeat: 5/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.2500\n","Precision: 0.4300\n","Recall: 0.2500\n","F1 Score: 0.2996\n","Log Loss: 2.5851\n","\n","Accuracy: 0.4000\n","Precision: 0.4000\n","Recall: 0.4000\n","F1 Score: 0.4000\n","Log Loss: 2.0224\n","\n","Accuracy: 0.1500\n","Precision: 0.1650\n","Recall: 0.1500\n","F1 Score: 0.1571\n","Log Loss: 2.7967\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.3000\n","Precision: 0.2773\n","Recall: 0.3529\n","F1 Score: 0.3106\n","Log Loss: 2.8363\n","\n","Accuracy: 0.3500\n","Precision: 0.3000\n","Recall: 0.3500\n","F1 Score: 0.3231\n","Log Loss: 1.9702\n","\n","Average Metrics for Repetition 5/10:\n","Accuracy: 0.2900\n","Precision: 0.3145\n","Recall: 0.3006\n","F1_score: 0.2981\n","Log_loss: 2.4422\n","\n","Cross-Validation Repeat: 6/10\n","\n","Accuracy: 0.3500\n","Precision: 0.3929\n","Recall: 0.5000\n","F1 Score: 0.4400\n","Log Loss: 2.5655\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:07] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.4115\n","Recall: 0.6667\n","F1 Score: 0.5033\n","Log Loss: 2.6900\n","\n","Accuracy: 0.1500\n","Precision: 0.2014\n","Recall: 0.1500\n","F1 Score: 0.1638\n","Log Loss: 2.6405\n","\n","Accuracy: 0.4000\n","Precision: 0.3887\n","Recall: 0.4211\n","F1 Score: 0.4042\n","Log Loss: 1.7713\n","\n","Accuracy: 0.4500\n","Precision: 0.5850\n","Recall: 0.4500\n","F1 Score: 0.5000\n","Log Loss: 1.9304\n","\n","Average Metrics for Repetition 6/10:\n","Accuracy: 0.3500\n","Precision: 0.3959\n","Recall: 0.4375\n","F1_score: 0.4023\n","Log_loss: 2.3195\n","\n","Cross-Validation Repeat: 7/10\n","\n","Accuracy: 0.4500\n","Precision: 0.6136\n","Recall: 0.4500\n","F1 Score: 0.5192\n","Log Loss: 1.6716\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.3875\n","Recall: 0.4000\n","F1 Score: 0.3800\n","Log Loss: 1.8494\n","\n","Accuracy: 0.4500\n","Precision: 0.4559\n","Recall: 0.5294\n","F1 Score: 0.4880\n","Log Loss: 1.8297\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.2500\n","Precision: 0.2083\n","Recall: 0.2500\n","F1 Score: 0.2273\n","Log Loss: 2.5108\n","\n","Accuracy: 0.4500\n","Precision: 0.4400\n","Recall: 0.6000\n","F1 Score: 0.5077\n","Log Loss: 2.2397\n","\n","Average Metrics for Repetition 7/10:\n","Accuracy: 0.4000\n","Precision: 0.4211\n","Recall: 0.4459\n","F1_score: 0.4244\n","Log_loss: 2.0202\n","\n","Cross-Validation Repeat: 8/10\n","\n","Accuracy: 0.2500\n","Precision: 0.2000\n","Recall: 0.2500\n","F1 Score: 0.2000\n","Log Loss: 2.9651\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:08] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.3981\n","Recall: 0.4000\n","F1 Score: 0.3960\n","Log Loss: 1.9711\n","\n","Accuracy: 0.5000\n","Precision: 0.4333\n","Recall: 0.5000\n","F1 Score: 0.4643\n","Log Loss: 1.8502\n","\n","Accuracy: 0.3000\n","Precision: 0.2538\n","Recall: 0.3000\n","F1 Score: 0.2750\n","Log Loss: 2.4073\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.2000\n","Precision: 0.5000\n","Recall: 0.2000\n","F1 Score: 0.2857\n","Log Loss: 2.4318\n","\n","Average Metrics for Repetition 8/10:\n","Accuracy: 0.3300\n","Precision: 0.3571\n","Recall: 0.3300\n","F1_score: 0.3242\n","Log_loss: 2.3251\n","\n","Cross-Validation Repeat: 9/10\n","\n","Accuracy: 0.2500\n","Precision: 0.3000\n","Recall: 0.2500\n","F1 Score: 0.2727\n","Log Loss: 2.0926\n","\n","Accuracy: 0.3500\n","Precision: 0.3467\n","Recall: 0.3500\n","F1 Score: 0.3393\n","Log Loss: 2.7118\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.3100\n","Recall: 0.4000\n","F1 Score: 0.3292\n","Log Loss: 2.1730\n","\n","Accuracy: 0.3500\n","Precision: 0.3500\n","Recall: 0.3500\n","F1 Score: 0.3500\n","Log Loss: 2.1812\n","\n","Accuracy: 0.4500\n","Precision: 0.4846\n","Recall: 0.4500\n","F1 Score: 0.4667\n","Log Loss: 1.7315\n","\n","Average Metrics for Repetition 9/10:\n","Accuracy: 0.3600\n","Precision: 0.3583\n","Recall: 0.3600\n","F1_score: 0.3516\n","Log_loss: 2.1780\n","\n","Cross-Validation Repeat: 10/10\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:09] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.1500\n","Precision: 0.2786\n","Recall: 0.1500\n","F1 Score: 0.1950\n","Log Loss: 2.2762\n","\n","Accuracy: 0.1500\n","Precision: 0.0923\n","Recall: 0.1500\n","F1 Score: 0.1143\n","Log Loss: 2.6970\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:10] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4000\n","Precision: 0.6000\n","Recall: 0.4000\n","F1 Score: 0.4625\n","Log Loss: 1.7724\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:10] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.4500\n","Precision: 0.3737\n","Recall: 0.5294\n","F1 Score: 0.4381\n","Log Loss: 2.1137\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.3500\n","Precision: 0.3300\n","Recall: 0.3500\n","F1 Score: 0.3389\n","Log Loss: 2.0329\n","\n","Average Metrics for Repetition 10/10:\n","Accuracy: 0.3000\n","Precision: 0.3349\n","Recall: 0.3159\n","F1_score: 0.3098\n","Log_loss: 2.1785\n","\n","Results saved to xgboostPCACV_results.csv\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Boxplot saved to XGBoostPCACV.png\n"]}]},{"cell_type":"code","source":["# Training and Predicting\n","\n","x_train_pca = x_train_pca\n","print(x_train_pca.head())\n","y_train = y_train\n","train_id = train_id\n","\n","x_test_pca = x_test_pca\n","y_test = y_test\n","test_id = test_id\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier  # Import XGBClassifier\n","from sklearn.metrics import accuracy_score\n","\n","\n","def xgboost_predict_proba(x_train, y_train, x_test):\n","    '''\n","    Trains an XGBoost classifier on the training data, outputs validation accuracy,\n","    and returns predicted probabilities for the test set with numeric class labels.\n","\n","    Parameters:\n","    x_train: Training features\n","    y_train: Training labels (starting from 1)\n","    x_test: Test features for which to predict probabilities\n","\n","    Returns:\n","    predicted_df: A DataFrame with predicted probabilities for each class (numeric labels)\n","    validation_accuracy: Accuracy score on the validation set\n","    '''\n","\n","    # Adjust y_train to be 0-based for internal processing\n","    y_train_adjusted = y_train - 1  # Convert to 0-based indexing\n","\n","    # Check the number of unique classes in y_train\n","    unique_classes = np.unique(y_train_adjusted)\n","\n","    # If there's only one unique class or the minimum class size is 1, we cannot stratify\n","    if len(unique_classes) < 2 or (y_train_adjusted.value_counts() < 2).any():\n","        print(\"Not enough classes for stratification, using random train/test split.\")\n","        stratify = None\n","    else:\n","        stratify = y_train_adjusted\n","\n","    # Split the data into training and validation sets (80% train, 20% validation)\n","    x_train_split, x_val, y_train_split, y_val = train_test_split(\n","        x_train, y_train_adjusted, test_size=0.2, random_state=42, stratify=stratify\n","    )\n","\n","    # Initialize the XGBoost model with default parameters\n","    xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n","\n","    # Train the model on the training split\n","    xgb_model.fit(x_train_split, y_train_split)\n","\n","    # Predict on the validation set\n","    y_val_pred = xgb_model.predict(x_val)\n","\n","    # Calculate validation accuracy\n","    validation_accuracy = accuracy_score(y_val, y_val_pred)\n","    print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n","\n","    # Predict probabilities on the test set\n","    y_pred_probs = xgb_model.predict_proba(x_test)\n","\n","    # Create a DataFrame for predicted probabilities with numeric class labels\n","    num_classes = 15  # Assuming classes are from 1 to 15\n","    predicted_df = pd.DataFrame(0, index=np.arange(y_pred_probs.shape[0]), columns=np.arange(1, num_classes + 1))\n","\n","    # Fill the DataFrame with the predicted probabilities\n","    for i in range(y_pred_probs.shape[1]):  # Iterate through the predicted probabilities\n","        predicted_df[i + 1] = y_pred_probs[:, i]  # Fill in probabilities for classes 1 to 15\n","\n","    # Handle any class mismatch\n","    if predicted_df.shape[1] < num_classes:\n","        print(f\"Warning: {predicted_df.shape[1]} classes found, expected {num_classes}. Imputing missing classes with zeros.\")\n","        for cls in range(1, num_classes + 1):\n","            if cls not in predicted_df.columns:\n","                predicted_df[cls] = 0.0  # Assign zero probability to missing classes\n","\n","    return predicted_df, validation_accuracy\n","\n","# Call the function\n","y_pred_probs, validation_accuracy = xgboost_predict_proba(x_train_pca, y_train, x_test_pca)\n","\n","if y_pred_probs is not None:\n","    y_pred_df = y_pred_probs.copy()\n","    y_pred_df.insert(0, 'Animal.ID', test_id.reset_index(drop=True))\n","    print(y_pred_df.head())\n","    y_pred_df.to_csv(\"XGBPCAy_pred_df.csv\", index=False)\n","\n","    y_test_df = pd.DataFrame({\n","        'Animal.ID': test_id.reset_index(drop=True),\n","        'Outcome.Type': y_test.reset_index(drop=True)  # No need for inverse mapping\n","    })\n","\n","    print(y_test_df.head())\n","\n","    # Final Scoring\n","    print(\"XGBoost with PCA\")\n","    xgboost_score = scoring_function(y_pred_df, y_test_df)\n","\n","    # Convert the dictionary to a DataFrame\n","    xgboost_score_df = pd.DataFrame([xgboost_score])\n","\n","    # Save the DataFrame to a CSV file\n","    xgboost_score_df.to_csv(\"xgboost_score_pca.csv\", index=False)\n","    print(\"XGBoostPCA score saved to 'xgboost_score.csv'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sA4MO-QIhOBt","executionInfo":{"status":"ok","timestamp":1728523992367,"user_tz":-660,"elapsed":8,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"a8bcb613-90ea-4cfd-aec2-d6e873d93560"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["             0\n","0  6564.294622\n","1  6564.294623\n","2  3849.294612\n","3 -8330.705179\n","4 -7900.705211\n","Validation Accuracy: 0.4500\n","  Animal.ID         1         2         3         4  5  6  7  8  9  10  11  \\\n","0   A454956  0.934876  0.028210  0.019670  0.017244  0  0  0  0  0   0   0   \n","1   A478575  0.934876  0.028210  0.019670  0.017244  0  0  0  0  0   0   0   \n","2   A478962  0.106638  0.836242  0.022015  0.035105  0  0  0  0  0   0   0   \n","3   A480389  0.933290  0.035580  0.006602  0.024529  0  0  0  0  0   0   0   \n","4   A495162  0.884340  0.068743  0.024499  0.022418  0  0  0  0  0   0   0   \n","\n","   12  13  14  15  \n","0   0   0   0   0  \n","1   0   0   0   0  \n","2   0   0   0   0  \n","3   0   0   0   0  \n","4   0   0   0   0  \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [01:33:11] WARNING: /workspace/src/learner.cc:740: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  warnings.warn(smsg, UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["  Animal.ID  Outcome.Type\n","0   A454956             1\n","1   A478575             1\n","2   A478962             3\n","3   A480389             1\n","4   A495162             7\n","XGBoost with PCA\n","\n","Accuracy: 0.2500\n","Precision: 0.2614\n","Recall: 0.2680\n","F1 Score: 0.2138\n","Log Loss: 4.9496\n","XGBoostPCA score saved to 'xgboost_score.csv'\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2956: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n","  warnings.warn(\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}