{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vBxUkBy2SMnx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728689133396,"user_tz":-660,"elapsed":8481,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"ae9fdf78-79a0-48f8-f16f-872be9460088"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Animal.ID    Name Animal.Type                        Breed            Sex  \\\n","0   A006100   Scamp         Dog         Spinone Italiano Mix  Neutered Male   \n","1   A006100   Scamp         Dog         Spinone Italiano Mix  Neutered Male   \n","2   A047759    Oreo         Dog                    Dachshund  Neutered Male   \n","3   A134067  Bandit         Dog            Shetland Sheepdog  Neutered Male   \n","4   A141142  Bettie         Dog  Labrador Retriever/Pit Bull  Spayed Female   \n","\n","          Color   Age      Intake.Type     Outcome.Type          Intake.Date  \\\n","0  Yellow/White   7.0    Public Assist  Return to Owner  2014-12-19 10:21:00   \n","1  Yellow/White   6.0    Public Assist  Return to Owner  2014-03-07 14:26:00   \n","2      Tricolor  10.0  Owner Surrender         Transfer  2014-04-02 15:55:00   \n","3   Brown/White  16.0    Public Assist  Return to Owner  2013-11-16 09:02:00   \n","4   Black/White  15.0            Stray  Return to Owner  2013-11-16 14:46:00   \n","\n","          Outcome.Date   State  \n","0  2014-12-20 16:35:00  Austin  \n","1  2014-03-08 17:10:00  Austin  \n","2  2014-04-07 15:12:00  Austin  \n","3  2013-11-16 11:54:00  Austin  \n","4  2013-11-17 11:40:00  Austin  \n"]}],"source":["# Remember to upload optipaw_FINAL.csv\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Set options to display all unique values\n","# pd.set_option('display.max_rows', None)  # Display all rows\n","# pd.set_option('display.max_columns', None)  # Display all columns\n","\n","# Load Optipaw Data\n","optipaw_data = pd.read_csv('optipaw_FINAL.csv')\n","\n","# Extract rows where the 'State' column is 'Austin'\n","austin_data = optipaw_data[optipaw_data['State'] == 'Austin'].copy()\n","\n","# Reset the index for the Austin data (optional but useful for clean DataFrames)\n","austin_data.reset_index(drop=True, inplace=True)\n","\n","# Display the first few rows of the extracted data\n","print(austin_data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"iutf7TsHTqtK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728689170546,"user_tz":-660,"elapsed":37154,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"e4bb7c15-3b60-4e2d-fdca-0785d05e7a15"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Animal.ID   Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type   State  \\\n","0   A006100  19560            1    1   7.0            1             1  Austin   \n","1   A006100  19560            1    1   6.0            1             1  Austin   \n","2   A047759  16845            1    1  10.0            2             2  Austin   \n","3   A134067   4665            1    1  16.0            1             1  Austin   \n","4   A141142   5095            1    2  15.0            3             1  Austin   \n","\n","   Abyssinian  Affenpinscher  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0           0              0  ...          19            12         2014   \n","1           0              0  ...           7             3         2014   \n","2           0              0  ...           2             4         2014   \n","3           0              0  ...          16            11         2013   \n","4           0              0  ...          16            11         2013   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           20             12          2014           10            16   \n","1            8              3          2014           14            17   \n","2            7              4          2014           15            15   \n","3           16             11          2013            9            11   \n","4           17             11          2013           14            11   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.617994              4.188790  \n","1             3.665191              4.450590  \n","2             3.926991              3.926991  \n","3             2.356194              2.879793  \n","4             3.665191              2.879793  \n","\n","[5 rows x 436 columns]\n"]}],"source":["# Preprocessing Function for ML\n","def preprocessing(df, name_mapping_var=None):\n","\n","  # For Name, we will use label encoding to assign each unique name a specific int, at the same time return the mapping\n","  # Strip leading asterisks from the Name column\n","  df['Name'] = df['Name'].str.lstrip('*')\n","  label_encoder = LabelEncoder()\n","  df['Name'] = label_encoder.fit_transform(df['Name'].astype(str))\n","\n","  # If the user provided a variable to store the mapping, assign it\n","  if name_mapping_var is not None:\n","      name_mapping = {k: v for v, k in enumerate(label_encoder.classes_)}\n","      name_mapping_var.update(name_mapping)\n","\n","\n","  # For Animal.Type we will map Int Values to the specific animal type\n","  animal_mapping = {\n","    \"Dog\": 1, \"Cat\": 2, \"Other\": 3, \"Bird\": 4, \"Livestock\": 5,\n","    \"House Rabbit\": 6, \"Rat\": 7, \"Ferret\": 8, \"Pig\": 9, \"Hamster\": 10,\n","    \"Guinea Pig\": 11, \"Gerbil\": 12, \"Hedgehog\": 13, \"Chinchilla\": 14,\n","    \"Goat\": 15, \"Mouse\": 16, \"Sugar Glider\": 17, \"Snake\": 18,\n","    \"Wildlife\": 19, \"Lizard\": 20\n","    }\n","\n","  df['Animal.Type'] = df['Animal.Type'].map(animal_mapping)\n","\n","  # For Breed we will perform one hot encoding onto it\n","  # Remove parentheses and their contents, and replace '/' with space\n","  df['Breed'] = df['Breed'].str.replace(r'\\(.*?\\)', '', regex=True).str.replace('/', ' ').str.replace(',', ' ')\n","\n","  # Split the 'Breed' column into a list and capitalize the first letter of each word\n","  df['Breed'] = df['Breed'].str.split().apply(lambda breeds: [breed.rstrip('-').capitalize() for breed in breeds])\n","\n","  # Create dummy variables for each unique breed\n","  df_breeds = df['Breed'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded breed dataframe\n","  df = pd.concat([df, df_breeds], axis=1)\n","\n","  # Drop the original 'Breed' column\n","  df = df.drop(columns=['Breed'])\n","\n","  # For Sex we will map Int Values to specific Sex\n","  sex_mapping = {'Neutered Male': 1, 'Spayed Female': 2, 'Intact Female': 3, 'Intact Male': 4, 'Unknown': 5, 'Female': 6, 'Male': 7}\n","\n","  # Map the Sex column using the defined mapping\n","  df['Sex'] = df['Sex'].map(sex_mapping)\n","\n","  # For colours, we will split into individual colours and use one hot encoding, which is assigning binary values to it\n","\n","  # Split the 'Color' column by '/', 'and', and ','\n","  df['Color'] = df['Color'].str.replace('/', ' ').str.replace('and', ' ').str.replace(',', ' ').str.replace(r'-\\b', '', regex=True)\n","\n","  # Split the 'Color' column into a list and capitalize the first letter of each word\n","  df['Color'] = df['Color'].str.split().apply(lambda colors: [color.capitalize() for color in colors])\n","\n","  # Create dummy variables for each unique color\n","  df_colors = df['Color'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded color dataframe\n","  df = pd.concat([df, df_colors], axis=1)\n","\n","  # Drop the original 'Color' column\n","  df = df.drop(columns=['Color'])\n","\n","  # For Age, we will just store it as int and impute it with 0 if it is null, and store it as float\n","  df['Age'] = df['Age'].fillna(0).astype(float)\n","\n","  # For Intake.Type, we will map Int Values to specific Intake\n","  intake_type_mapping = {\n","      'Public Assist': 1, 'Owner Surrender': 2, 'Stray': 3, 'Euthanasia Request': 4,\n","      'Abandoned': 5, 'Wildlife': 6, 'Moving': 7, 'Incompatible with owner lifestyle': 8,\n","      'Rabies Monitoring': 9, 'Marriage/Relationship split': 10, 'Owner Deceased': 11, 'Police Assist': 12,\n","      'Biting': 13, 'Owner Died': 14, 'TNR - Trap/Neuter/Release': 15, 'Unable to Afford': 16,\n","      'Unsuitable Accommodation': 17, 'Allergies': 18, 'Transfer from Other Shelter': 19,\n","      'Born in Shelter': 20, 'Landlord issues': 21, 'Litter relinquishment': 22, 'Sick/Injured': 23,\n","      'Owner requested Euthanasia': 24, 'Abuse/ neglect': 25, 'Incompatible with other pets': 26,\n","      'Behavioral Issues': 27, 'DOA': 28\n","  }\n","\n","  # Map the Intake.Type column using the defined mapping\n","  df['Intake.Type'] = df['Intake.Type'].map(intake_type_mapping)\n","\n","  # For Outcome.Type, we will map Int Values to specific Outcome\n","  outcome_type_mapping = {\n","      'Return to Owner': 1, 'Transfer': 2, 'Adoption': 3, 'Euthanasia': 4,\n","      'Died': 5, 'Rto-Adopt': 6, 'Disposal': 7, 'Missing': 8,\n","      'Stolen': 9, 'Relocate': 10, 'Lost': 11, 'Foster': 12,\n","      'Reclaimed': 13, 'Escaped': 14, 'Released To Wild': 15\n","  }\n","\n","  # Map the Outcome.Type column using the defined mapping\n","  df['Outcome.Type'] = df['Outcome.Type'].map(outcome_type_mapping)\n","\n","  # For Date and Time, we will be using panda and numpy date conversion\n","\n","  # Convert Intake.Date and Outcome.Date to datetime format\n","  df['Intake.Date'] = pd.to_datetime(df['Intake.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","  df['Outcome.Date'] = pd.to_datetime(df['Outcome.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","\n","  # Extract date components from the date columns\n","  df['Intake.Day'] = df['Intake.Date'].dt.day.fillna(0).astype(int)\n","  df['Intake.Month'] = df['Intake.Date'].dt.month.fillna(0).astype(int)\n","  df['Intake.Year'] = df['Intake.Date'].dt.year.fillna(0).astype(int)\n","\n","  df['Outcome.Day'] = df['Outcome.Date'].dt.day.fillna(0).astype(int)\n","  df['Outcome.Month'] = df['Outcome.Date'].dt.month.fillna(0).astype(int)\n","  df['Outcome.Year'] = df['Outcome.Date'].dt.year.fillna(0).astype(int)\n","\n","  # Extract and convert the hour to radians\n","  df['Intake.Hour'] = df['Intake.Date'].dt.hour.fillna(0).astype(int)\n","  df['Outcome.Hour'] = df['Outcome.Date'].dt.hour.fillna(0).astype(int)\n","\n","  df['Intake.Hour.Radians'] = (df['Intake.Hour'] / 24) * 2 * np.pi\n","  df['Outcome.Hour.Radians'] = (df['Outcome.Hour'] / 24) * 2 * np.pi\n","\n","  # Drop original date columns if no longer needed\n","  df = df.drop(columns=['Intake.Date', 'Outcome.Date'])\n","\n","  return df\n","\n","# Process Austin Dataset (Animal ID and State to be removed later)\n","austin_data = preprocessing(austin_data)\n","\n","print(austin_data.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"GtsN2gkuUgnk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728689170895,"user_tz":-660,"elapsed":369,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"f065a1fd-4bc5-42fd-97f2-6e3e1d5957ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Animal.ID                object\n","Name                      int64\n","Animal.Type               int64\n","Sex                       int64\n","Age                     float64\n","                         ...   \n","Outcome.Year              int64\n","Intake.Hour               int64\n","Outcome.Hour              int64\n","Intake.Hour.Radians     float64\n","Outcome.Hour.Radians    float64\n","Length: 436, dtype: object\n","(115498, 436)\n","Animal.ID               99642\n","Name                    23514\n","Animal.Type                 5\n","Sex                         5\n","Age                        42\n","                        ...  \n","Outcome.Year               12\n","Intake.Hour                24\n","Outcome.Hour               24\n","Intake.Hour.Radians        24\n","Outcome.Hour.Radians       24\n","Length: 436, dtype: int64\n","Animal.ID               0\n","Name                    0\n","Animal.Type             0\n","Sex                     0\n","Age                     0\n","                       ..\n","Outcome.Year            0\n","Intake.Hour             0\n","Outcome.Hour            0\n","Intake.Hour.Radians     0\n","Outcome.Hour.Radians    0\n","Length: 436, dtype: int64\n"]}],"source":["# Unique value checks and Null value checks\n","# Print dtypes, unique and missing value checks before splitting\n","print(austin_data.dtypes)\n","print(austin_data.shape)\n","print(austin_data.nunique())\n","print(austin_data.isnull().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"q1aUNY_PUusI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728689173246,"user_tz":-660,"elapsed":2358,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"1b84e769-214d-4cce-ef02-857f1658a8f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["    Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type  Abyssinian  \\\n","0  19560            1    1   7.0            1             1           0   \n","1  19560            1    1   6.0            1             1           0   \n","2  16845            1    1  10.0            2             2           0   \n","3   4665            1    1  16.0            1             1           0   \n","4   5095            1    2  15.0            3             1           0   \n","\n","   Affenpinscher  Afghan  African  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0              0       0        0  ...          19            12         2014   \n","1              0       0        0  ...           7             3         2014   \n","2              0       0        0  ...           2             4         2014   \n","3              0       0        0  ...          16            11         2013   \n","4              0       0        0  ...          16            11         2013   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           20             12          2014           10            16   \n","1            8              3          2014           14            17   \n","2            7              4          2014           15            15   \n","3           16             11          2013            9            11   \n","4           17             11          2013           14            11   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.617994              4.188790  \n","1             3.665191              4.450590  \n","2             3.926991              3.926991  \n","3             2.356194              2.879793  \n","4             3.665191              2.879793  \n","\n","[5 rows x 434 columns]\n","    Name  Animal.Type  Sex   Age  Intake.Type  Outcome.Type  Abyssinian  \\\n","0   9626            2    1  16.0            3             1           0   \n","1  20023            1    2  16.0            1             1           0   \n","2   4940            2    2  16.0            3             3           0   \n","3   9385            2    1  16.0            3             1           0   \n","4  15780            2    1  16.0            1             7           0   \n","\n","   Affenpinscher  Afghan  African  ...  Intake.Day  Intake.Month  Intake.Year  \\\n","0              0       0        0  ...          25             8         2023   \n","1              0       0        0  ...          27             4         2023   \n","2              0       0        0  ...          28             3         2023   \n","3              0       0        0  ...          23            10         2023   \n","4              0       0        0  ...          11             5         2024   \n","\n","   Outcome.Day  Outcome.Month  Outcome.Year  Intake.Hour  Outcome.Hour  \\\n","0           25              8          2023           11            14   \n","1           28              4          2023           14            12   \n","2           17              4          2023           20            11   \n","3           24             10          2023           18            13   \n","4           26              5          2024           12             9   \n","\n","   Intake.Hour.Radians  Outcome.Hour.Radians  \n","0             2.879793              3.665191  \n","1             3.665191              3.141593  \n","2             5.235988              2.879793  \n","3             4.712389              3.403392  \n","4             3.141593              2.356194  \n","\n","[5 rows x 434 columns]\n","0    A006100\n","1    A006100\n","2    A047759\n","3    A134067\n","4    A141142\n","Name: Animal.ID, dtype: object\n","0    A454956\n","1    A478575\n","2    A478962\n","3    A480389\n","4    A495162\n","Name: Animal.ID, dtype: object\n","(101615, 434)\n","(13883, 434)\n"]}],"source":["# Splitting austin_data based on Outcome.Year\n","train_data = austin_data[austin_data['Outcome.Year'] < 2023].copy()\n","test_data = austin_data[austin_data['Outcome.Year'] >= 2023].copy()\n","\n","# Reset the index for both datasets\n","train_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)\n","\n","# Drop 'State' column from both train and test datasets\n","train_data = train_data.drop(columns=['State'])\n","test_data = test_data.drop(columns=['State'])\n","\n","# Extract Animal.ID from the train_data and test_data\n","train_ids = train_data['Animal.ID']\n","test_ids = test_data['Animal.ID']\n","\n","# Drop 'Animal.ID' from train_data and test_data\n","train_data = train_data.drop(columns=['Animal.ID'])\n","test_data = test_data.drop(columns=['Animal.ID'])\n","\n","# Output the result to verify the split\n","print(train_data.head(5))\n","print(test_data.head(5))\n","\n","print(train_ids.head(5))\n","print(test_ids.head(5))\n","\n","print(train_data.shape)\n","print(test_data.shape)\n","\n","# Split dataset to x_train, y_train and x_test, y_test\n","x_train = train_data.drop(columns=['Outcome.Type'])\n","y_train = train_data['Outcome.Type']\n","train_id = train_ids\n","\n","x_test = test_data.drop(columns=['Outcome.Type'])\n","y_test = test_data['Outcome.Type']\n","test_id = test_ids\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrDRvLXwYNeb","collapsed":true},"outputs":[],"source":["# Scoring Function\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","import numpy as np\n","import pandas as pd\n","\n","def scoring_function(y_pred_df, y_test_df, all_classes=None):\n","    '''\n","    Calculates various performance metrics for the predictions.\n","\n","    Parameters:\n","    y_pred_df: DataFrame containing predictions.\n","    y_test_df: DataFrame containing true labels.\n","    all_classes: List of all possible classes.\n","\n","    Returns:\n","    metrics: Dictionary containing accuracy, precision, recall, f1 score, and log loss.\n","    '''\n","\n","    # Merge the prediction and actual outcome dataframes using Animal ID\n","    df_combined = pd.merge(y_pred_df, y_test_df, on='Animal.ID')\n","\n","    if df_combined.empty:\n","        print(\"No data in the combined DataFrame.\")\n","        return None\n","\n","    # Convert the predicted probabilities into predicted classes (argmax)\n","    y_pred_class = df_combined.iloc[:, 1:-1].idxmax(axis=1).astype(int)\n","    y_true = df_combined['Outcome.Type'].astype(int)\n","\n","    # If all_classes is None, use unique classes from y_true\n","    if all_classes is None:\n","        all_classes = np.unique(y_true)  # Get unique classes from y_true\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_true, y_pred_class)\n","\n","    # Calculate metrics only for predicted classes\n","    unique_pred_classes = np.unique(y_pred_class)\n","    precision = precision_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    recall = recall_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    f1 = f1_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","\n","    # Ensure that y_pred_probs contains probabilities for all classes\n","    y_pred_probs = df_combined.iloc[:, 1:-1].values\n","\n","    # Create an array to ensure all classes are represented in predictions\n","    complete_probs = np.zeros((y_pred_probs.shape[0], len(all_classes)))  # Initialize with zeros\n","    for i, class_label in enumerate(all_classes):\n","        if class_label in df_combined.columns:\n","            complete_probs[:, i] = y_pred_probs[:, class_label - 1]  # Fill with actual probabilities\n","\n","    # Calculate log loss with all classes present in the labels\n","    logloss = log_loss(y_true, complete_probs, labels=all_classes)\n","\n","    # Print the metrics\n","    print(f\"\\nAccuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Log Loss: {logloss:.4f}\")\n","\n","    # Return the metrics as a dictionary\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,\n","        'log_loss': logloss\n","    }\n","\n","\n"]},{"cell_type":"code","source":["import warnings\n","\n","# Suppress FutureWarnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import KFold\n","from sklearn.svm import SVC\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","\n","def perform_cross_validation(x_train, y_train, scoring_function, n_splits=5, n_repeats=10, save_path=\"svm_cv_results.csv\"):\n","    \"\"\"\n","    Perform repeated cross-validation using SVM, collect the scoring metrics, and save results to a CSV.\n","\n","    Parameters:\n","    x_train: Training features\n","    y_train: Training labels (starting from 1)\n","    scoring_function: Function to compute the scoring metrics\n","    n_splits: Number of splits for K-fold cross-validation (default is 5)\n","    n_repeats: Number of times to repeat the cross-validation (default is 10)\n","    save_path: Path to save the CSV file (default is 'svm_cv_results.csv')\n","\n","    Returns:\n","    results: A dictionary of lists where each list contains average values for the metrics (one per repetition)\n","    \"\"\"\n","    # Convert y_train to 0-based indexing\n","    y_train_adjusted = y_train - 1  # Adjust to 0-based indexing for internal processing\n","\n","    # Get all unique classes from the full y_train\n","    all_classes = np.unique(y_train_adjusted)\n","\n","    # Store metrics for all repetitions\n","    results = {\n","        'accuracy': [],\n","        'precision': [],\n","        'recall': [],\n","        'f1_score': [],\n","        'log_loss': []\n","    }\n","\n","    # DataFrame to hold all metrics across repetitions\n","    df_results = pd.DataFrame()\n","\n","    # Repeat the cross-validation process n_repeats times\n","    for repeat in range(n_repeats):\n","        print(f\"\\nCross-Validation Repeat: {repeat + 1}/{n_repeats}\")\n","\n","        # Use a different random_state for KFold in each repetition to introduce variability\n","        kf = KFold(n_splits=n_splits, shuffle=True, random_state=repeat)\n","\n","        # Store metrics for each fold within this repetition\n","        fold_metrics = {\n","            'accuracy': [],\n","            'precision': [],\n","            'recall': [],\n","            'f1_score': [],\n","            'log_loss': []\n","        }\n","\n","        # Perform K-fold cross-validation\n","        for train_idx, val_idx in kf.split(x_train):\n","            x_fold_train, x_fold_val = x_train.iloc[train_idx], x_train.iloc[val_idx]\n","            y_fold_train, y_fold_val = y_train_adjusted.iloc[train_idx], y_train_adjusted.iloc[val_idx]  # Use adjusted labels\n","\n","            try:\n","                # Train an SVM model with max_iter set to 1000\n","                svm_model = SVC(probability=True, random_state=42, max_iter=1000, tol=1e-3, kernel='linear')  # Enable probability estimates\n","                svm_model.fit(x_fold_train, y_fold_train)\n","\n","                # Predict probabilities for validation set\n","                y_val_pred_probs = svm_model.predict_proba(x_fold_val)\n","\n","                # Prepare the DataFrame for scoring\n","                y_pred_df = pd.DataFrame(y_val_pred_probs, columns=all_classes)\n","                y_pred_df.insert(0, 'Animal.ID', val_idx)  # Simulate Animal IDs using validation indices\n","                y_test_df = pd.DataFrame({\n","                    'Animal.ID': val_idx,\n","                    'Outcome.Type': y_fold_val.reset_index(drop=True) + 1  # Convert back to 1-based for the scoring function\n","                })\n","\n","                # Use the scoring function\n","                metrics = scoring_function(y_pred_df, y_test_df, all_classes=all_classes)\n","\n","                # Collect metrics for this fold\n","                for key in fold_metrics.keys():\n","                    fold_metrics[key].append(metrics[key])\n","\n","            except ValueError as e:\n","                if \"Invalid classes inferred from unique values of `y`\" in str(e):\n","                    continue  # Skip this fold if there's a class mismatch\n","                else:\n","                    continue  # Handle other errors as needed\n","\n","        # Compute the average score for each metric across folds in this repetition\n","        avg_metrics = {key: np.mean(fold_metrics[key]) for key in fold_metrics.keys()}\n","\n","        # Print the average scores for this repetition (including the repetition number)\n","        print(f\"\\nAverage Metrics for Repetition {repeat + 1}/{n_repeats}:\")\n","        for key, value in avg_metrics.items():\n","            print(f\"{key.capitalize()}: {value:.4f}\")\n","\n","        # Append the average scores for each repetition to the results\n","        for key in results.keys():\n","            results[key].append(avg_metrics[key])  # Append the average score for this repetition\n","\n","        # Add results of this repetition to a DataFrame (for saving to CSV)\n","        avg_metrics['repetition'] = repeat + 1  # Add repetition number\n","        df_results = pd.concat([df_results, pd.DataFrame([avg_metrics])], ignore_index=True)\n","\n","    # Save the DataFrame to a CSV file after all repetitions\n","    df_results.to_csv(save_path, index=False)\n","\n","    return results\n","\n","\n","# Function to plot the cross-validation results and save the plot as an image\n","def plot_cross_validation_results(results, save_path=\"SVMCV.png\"):\n","    \"\"\"\n","    Plot boxplots of the cross-validation results and save the plot as an image.\n","\n","    Parameters:\n","    results: A dictionary containing lists of metrics collected from cross-validation.\n","    save_path: Path to save the plot image (default is 'boxplot.png').\n","    \"\"\"\n","    # Convert results dictionary to DataFrame\n","    results_df = pd.DataFrame(results)\n","\n","    # Create the boxplot\n","    plt.figure(figsize=(10, 6))\n","    sns.boxplot(data=results_df)\n","    plt.title(\"SVM 5-Fold Cross-Validation Metrics\")\n","    plt.ylabel(\"Score\")\n","\n","    # Save the plot as an image file\n","    plt.savefig(save_path, bbox_inches='tight')\n","\n","    # Display the plot\n","    plt.show()\n","\n","    print(f\"Boxplot saved to {save_path}\")\n","\n"],"metadata":{"id":"mc5jWtxX4pRM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Edit Here\n","\n","x_train = x_train\n","y_train = y_train\n","train_id = train_id\n","\n","x_test = x_test\n","y_test = y_test\n","test_id = test_id\n","\n","# Call the cross-validation function\n","results = perform_cross_validation(x_train, y_train, scoring_function)\n","#print(results)\n","\n","# Plot the results\n","plot_cross_validation_results(results)"],"metadata":{"id":"RYqZfzzm6BdN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6478cc1-3410-42e0-8097-282575785894"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Cross-Validation Repeat: 1/10\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.1886\n","Precision: 0.1886\n","Recall: 1.0000\n","F1 Score: 0.3173\n","Log Loss: 1.1313\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.1843\n","Precision: 0.1843\n","Recall: 1.0000\n","F1 Score: 0.3113\n","Log Loss: 1.1198\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.1902\n","Precision: 0.1902\n","Recall: 1.0000\n","F1 Score: 0.3196\n","Log Loss: 1.1381\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","Average Metrics for Repetition 1/10:\n","Accuracy: 0.1877\n","Precision: 0.1877\n","Recall: 1.0000\n","F1_score: 0.3160\n","Log_loss: 1.1297\n","\n","Cross-Validation Repeat: 2/10\n"]}]},{"cell_type":"code","source":["# Training and Predicting\n","\n","x_train = x_train\n","print(x_train.head())\n","y_train = y_train\n","train_id = train_id\n","\n","x_test = x_test\n","y_test = y_test\n","test_id = test_id\n","\n","from sklearn.svm import SVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np\n","\n","def svm_predict_proba(x_train, y_train, x_test):\n","    '''\n","    Trains an SVM classifier on the training data, outputs validation accuracy,\n","    and returns predicted probabilities for the test set with numeric class labels.\n","\n","    Parameters:\n","    x_train: Training features\n","    y_train: Training labels\n","    x_test: Test features for which to predict probabilities\n","\n","    Returns:\n","    predicted_df: A DataFrame with predicted probabilities for each class (numeric labels)\n","    validation_accuracy: Accuracy score on the validation set\n","    '''\n","\n","    # Check for the least populated class and handle it\n","    class_counts = np.bincount(y_train)\n","    if np.min(class_counts) < 2:\n","        # If there's a class with fewer than 2 samples, impute that class with a duplicate or a neutral value\n","        for cls in range(len(class_counts)):\n","            if class_counts[cls] < 2:\n","                # Duplicate the available samples to ensure there are at least 2\n","                idx_to_duplicate = np.where(y_train == cls)[0]\n","                if len(idx_to_duplicate) > 0:\n","                    # Duplicate the first available index to create a new sample\n","                    new_sample = x_train.iloc[idx_to_duplicate[0]].copy()\n","                    y_train = np.append(y_train, cls)\n","                    x_train = pd.concat([x_train, new_sample.to_frame().T], ignore_index=True)\n","\n","    # Split the data into training and validation sets (80% train, 20% validation)\n","    x_train_split, x_val, y_train_split, y_val = train_test_split(\n","        x_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n","    )\n","\n","    # Initialize the SVM model with probability estimates\n","    svm_model = SVC(probability=True, random_state=42, max_iter=1000)\n","\n","    # Train the model on the training split\n","    svm_model.fit(x_train_split, y_train_split)\n","\n","    # Predict on the validation set\n","    y_val_pred = svm_model.predict(x_val)\n","\n","    # Calculate validation accuracy\n","    validation_accuracy = accuracy_score(y_val, y_val_pred)\n","\n","    # Predict probabilities on the test set\n","    y_pred_probs = svm_model.predict_proba(x_test)\n","\n","    # Create a DataFrame for predicted probabilities with numeric class labels\n","    num_classes = 15  # Assuming classes are from 1 to 15\n","    predicted_df = pd.DataFrame(0, index=np.arange(y_pred_probs.shape[0]), columns=np.arange(1, num_classes + 1))\n","\n","    # Fill the DataFrame with the predicted probabilities\n","    for i in range(y_pred_probs.shape[1]):  # Iterate through the predicted probabilities\n","        predicted_df[i + 1] = y_pred_probs[:, i]  # Fill in probabilities for classes 1 to 15\n","\n","    return predicted_df, validation_accuracy\n","\n","# Call the function\n","y_pred_probs, validation_accuracy = svm_predict_proba(x_train, y_train, x_test)\n","\n","# Continue with your existing code\n","if y_pred_probs is not None:\n","    y_pred_df = y_pred_probs.copy()\n","    y_pred_df.insert(0, 'Animal.ID', test_id.reset_index(drop=True))\n","    print(y_pred_df.head())\n","    y_pred_df.to_csv(\"SVMy_pred_df.csv\", index=False)\n","\n","    y_test_df = pd.DataFrame({\n","        'Animal.ID': test_id.reset_index(drop=True),\n","        'Outcome.Type': y_test.reset_index(drop=True)  # No need for inverse mapping\n","    })\n","\n","    print(y_test_df.head())\n","\n","    # Final Scoring\n","    print(\"SVM without PCA\")\n","    svm_score = scoring_function(y_pred_df, y_test_df)\n","\n","    # Convert the dictionary to a DataFrame\n","    svm_score_df = pd.DataFrame([svm_score])\n","\n","    # Save the DataFrame to a CSV file\n","    svm_score_df.to_csv(\"svm_score.csv\", index=False)\n","    print(\"SVM score saved to 'svm_score.csv'\")"],"metadata":{"id":"sA4MO-QIhOBt"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}