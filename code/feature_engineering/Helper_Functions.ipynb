{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhZEDnOf7dzui31F1emvZC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JYIu-i7jFnHw","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1727576856013,"user_tz":-600,"elapsed":39402,"user":{"displayName":"Ng YC","userId":"11737261317625661208"}},"outputId":"545a6a80-b286-4aa8-ec98-ac7b9a2a8a16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nFinal suggestion:\\nWhen training data, use x train and y train without the Animal ID (this ensures Animal ID won't be processed)\\nWhen predicting data, feed in x test and OBTAIN y pred (it will follow the order)\\nLastly, combine y pred with the test id (exam); same goes to y test and test id (mark scheme)\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["'''\n","Logs\n","26/9 12:00am - Split Animal ID instead deleting it\n","25/9 11:43pm - Finished basic implementation\n","'''\n","\n","# Preprocessing Function for dataset before applying ML\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Important note: preprocessing doesn't delete the Animal ID, you have to manually delete later on, reason is to preserve it\n","# as a variable for scoring\n","def preprocessing(df, name_mapping_var=None):\n","\n","  # For Name, we will use label encoding to assign each unique name a specific int, at the same time return the mapping\n","  # Strip leading asterisks from the Name column\n","  df['Name'] = df['Name'].str.lstrip('*')\n","  label_encoder = LabelEncoder()\n","  df['Name'] = label_encoder.fit_transform(df['Name'].astype(str))\n","\n","  # If the user provided a variable to store the mapping, assign it\n","  if name_mapping_var is not None:\n","      name_mapping = {k: v for v, k in enumerate(label_encoder.classes_)}\n","      name_mapping_var.update(name_mapping)\n","\n","\n","  # For Animal.Type we will map Int Values to the specific animal type\n","  animal_mapping = {\n","    \"Dog\": 1, \"Cat\": 2, \"Other\": 3, \"Bird\": 4, \"Livestock\": 5,\n","    \"House Rabbit\": 6, \"Rat\": 7, \"Ferret\": 8, \"Pig\": 9, \"Hamster\": 10,\n","    \"Guinea Pig\": 11, \"Gerbil\": 12, \"Hedgehog\": 13, \"Chinchilla\": 14,\n","    \"Goat\": 15, \"Mouse\": 16, \"Sugar Glider\": 17, \"Snake\": 18,\n","    \"Wildlife\": 19, \"Lizard\": 20\n","    }\n","\n","  df['Animal.Type'] = df['Animal.Type'].map(animal_mapping)\n","\n","  # For Breed we will perform one hot encoding onto it\n","  # Remove parentheses and their contents, and replace '/' with space\n","  df['Breed'] = df['Breed'].str.replace(r'\\(.*?\\)', '', regex=True).str.replace('/', ' ').str.replace(',', ' ')\n","\n","  # Split the 'Breed' column into a list and capitalize the first letter of each word\n","  df['Breed'] = df['Breed'].str.split().apply(lambda breeds: [breed.rstrip('-').capitalize() for breed in breeds])\n","\n","  # Create dummy variables for each unique breed\n","  df_breeds = df['Breed'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded breed dataframe\n","  df = pd.concat([df, df_breeds], axis=1)\n","\n","  # Drop the original 'Breed' column\n","  df = df.drop(columns=['Breed'])\n","\n","  # For Sex we will map Int Values to specific Sex\n","  sex_mapping = {'Neutered Male': 1, 'Spayed Female': 2, 'Intact Female': 3, 'Intact Male': 4, 'Unknown': 5, 'Female': 6, 'Male': 7}\n","\n","  # Map the Sex column using the defined mapping\n","  df['Sex'] = df['Sex'].map(sex_mapping)\n","\n","  # For colours, we will split into individual colours and use one hot encoding, which is assigning binary values to it\n","\n","  # Split the 'Color' column by '/', 'and', and ','\n","  df['Color'] = df['Color'].str.replace('/', ' ').str.replace('and', ' ').str.replace(',', ' ').str.replace(r'-\\b', '', regex=True)\n","\n","  # Split the 'Color' column into a list and capitalize the first letter of each word\n","  df['Color'] = df['Color'].str.split().apply(lambda colors: [color.capitalize() for color in colors])\n","\n","  # Create dummy variables for each unique color\n","  df_colors = df['Color'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded color dataframe\n","  df = pd.concat([df, df_colors], axis=1)\n","\n","  # Drop the original 'Color' column\n","  df = df.drop(columns=['Color'])\n","\n","  # For Age, we will just store it as int and impute it with 0 if it is null, and store it as float\n","  df['Age'] = df['Age'].fillna(0).astype(float)\n","\n","  # For Intake.Type, we will map Int Values to specific Intake\n","  intake_type_mapping = {\n","      'Public Assist': 1, 'Owner Surrender': 2, 'Stray': 3, 'Euthanasia Request': 4,\n","      'Abandoned': 5, 'Wildlife': 6, 'Moving': 7, 'Incompatible with owner lifestyle': 8,\n","      'Rabies Monitoring': 9, 'Marriage/Relationship split': 10, 'Owner Deceased': 11, 'Police Assist': 12,\n","      'Biting': 13, 'Owner Died': 14, 'TNR - Trap/Neuter/Release': 15, 'Unable to Afford': 16,\n","      'Unsuitable Accommodation': 17, 'Allergies': 18, 'Transfer from Other Shelter': 19,\n","      'Born in Shelter': 20, 'Landlord issues': 21, 'Litter relinquishment': 22, 'Sick/Injured': 23,\n","      'Owner requested Euthanasia': 24, 'Abuse/ neglect': 25, 'Incompatible with other pets': 26,\n","      'Behavioral Issues': 27, 'DOA': 28\n","  }\n","\n","  # Map the Intake.Type column using the defined mapping\n","  df['Intake.Type'] = df['Intake.Type'].map(intake_type_mapping)\n","\n","  # For Outcome.Type, we will map Int Values to specific Outcome\n","  outcome_type_mapping = {\n","      'Return to Owner': 1, 'Transfer': 2, 'Adoption': 3, 'Euthanasia': 4,\n","      'Died': 5, 'Rto-Adopt': 6, 'Disposal': 7, 'Missing': 8,\n","      'Stolen': 9, 'Relocate': 10, 'Lost': 11, 'Foster': 12,\n","      'Reclaimed': 13, 'Escaped': 14, 'Released To Wild': 15\n","  }\n","\n","  # Map the Outcome.Type column using the defined mapping\n","  df['Outcome.Type'] = df['Outcome.Type'].map(outcome_type_mapping)\n","\n","  # For Date and Time, we will be using panda and numpy date conversion\n","\n","  # Convert Intake.Date and Outcome.Date to datetime format\n","  df['Intake.Date'] = pd.to_datetime(df['Intake.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","  df['Outcome.Date'] = pd.to_datetime(df['Outcome.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","\n","  # Extract date components from the date columns\n","  df['Intake.Day'] = df['Intake.Date'].dt.day.fillna(0).astype(int)\n","  df['Intake.Month'] = df['Intake.Date'].dt.month.fillna(0).astype(int)\n","  df['Intake.Year'] = df['Intake.Date'].dt.year.fillna(0).astype(int)\n","\n","  df['Outcome.Day'] = df['Outcome.Date'].dt.day.fillna(0).astype(int)\n","  df['Outcome.Month'] = df['Outcome.Date'].dt.month.fillna(0).astype(int)\n","  df['Outcome.Year'] = df['Outcome.Date'].dt.year.fillna(0).astype(int)\n","\n","  # Extract and convert the hour to radians\n","  df['Intake.Hour'] = df['Intake.Date'].dt.hour.fillna(0).astype(int)\n","  df['Outcome.Hour'] = df['Outcome.Date'].dt.hour.fillna(0).astype(int)\n","\n","  df['Intake.Hour.Radians'] = (df['Intake.Hour'] / 24) * 2 * np.pi\n","  df['Outcome.Hour.Radians'] = (df['Outcome.Hour'] / 24) * 2 * np.pi\n","\n","  # Drop original date columns if no longer needed\n","  df = df.drop(columns=['Intake.Date', 'Outcome.Date'])\n","\n","  return df\n","\n","\n","optipaw_data = pd.read_csv('optipaw_FINAL.csv')\n","optipaw_data = preprocessing(optipaw_data)\n","\n","# # Example usage to get name dict\n","# optipaw_name = {} # define a dictionary\n","# optipaw_data = preprocessing(optipaw_data, optipaw_name)\n","# print(optipaw_name)\n","\n","# Set options to display all unique values\n","pd.set_option('display.max_rows', None)  # Display all rows\n","pd.set_option('display.max_columns', None)  # Display all columns\n","\n","# # Print dtypes, unique and missing value checks before splitting\n","# print(optipaw_data.dtypes)\n","# print(optipaw_data.shape)\n","# print(optipaw_data.nunique())\n","# print(optipaw_data.isnull().sum())\n","\n","# Split the dataset into training (Austin) and also test (non Austin)\n","\n","train_data = optipaw_data[optipaw_data['State'] == 'Austin'].copy()\n","test_data = optipaw_data[optipaw_data['State'] != 'Austin'].copy()\n","\n","# Reset the index for both datasets\n","train_data.reset_index(drop=True, inplace=True)\n","test_data.reset_index(drop=True, inplace=True)\n","\n","# Drop 'State' column from both train and test datasets\n","train_data = train_data.drop(columns=['State'])\n","test_data = test_data.drop(columns=['State'])\n","\n","# print(train_data.head(5))\n","# print(test_data.head(5))\n","\n","# train_data.head(1000).to_csv('a.csv', index=False)\n","# test_data.head(1000).to_csv('b.csv', index=False)\n","\n","# Note that the preprocessing doesn't delete the Animal ID, reason is to preserve it for scoring\n","# Extract Animal.ID from the train_data and test_data\n","train_ids = train_data['Animal.ID']\n","test_ids = test_data['Animal.ID']\n","\n","# Drop 'Animal.ID' from train_data and test_data\n","train_data = train_data.drop(columns=['Animal.ID'])\n","test_data = test_data.drop(columns=['Animal.ID'])\n","\n","# print(train_data.head(5))\n","# print(test_data.head(5))\n","# print(train_ids.head(5))\n","# print(test_ids.head(5))\n","\n","\n","# # Additional print dtypes, unique and missing values for both train and test dataset\n","# print(train_data.dtypes)\n","# print(train_data.shape)\n","# print(train_data.nunique())\n","# print(train_data.isnull().sum())\n","\n","# print(test_data.dtypes)\n","# print(test_data.shape)\n","# print(test_data.nunique())\n","# print(test_data.isnull().sum())\n","\n","# # Sample Output for viewing\n","# train_data.head(1000).to_csv('train.csv', index=False)\n","# test_data.head(1000).to_csv('test.csv', index=False)\n","\n","\n","# Example Split\n","x_train = train_data.drop(columns=['Outcome.Type'])\n","y_train = train_data['Outcome.Type']\n","train_id = train_ids\n","\n","x_test = test_data.drop(columns=['Outcome.Type'])\n","y_test = test_data['Outcome.Type']\n","test_id = test_ids\n","\n","# print(\"train\")\n","# print(train_id.head(10))\n","# print(y_train.head(10))\n","# print(\"test\")\n","# print(test_id.head(10))\n","# print(y_test.head(10))\n","\n","\n","# # Build the Random Forest model - to check whether preprocess works and can fit into a ML\n","# from sklearn.ensemble import RandomForestClassifier\n","\n","# rftrain_model = RandomForestClassifier()\n","# rftrain_model.fit(x_train, y_train)\n","\n","# rftest_model = RandomForestClassifier()\n","# rftest_model.fit(x_test, y_test)\n","\n","'''\n","Final suggestion:\n","When training data, use x train and y train without the Animal ID (this ensures Animal ID won't be processed)\n","When predicting data, feed in x test and OBTAIN y pred (it will follow the order)\n","Lastly, combine y pred with the test id (exam); same goes to y test and test id (mark scheme)\n","'''\n"]},{"cell_type":"code","source":["'''\n","Logging\n","5/10 20.11 - Updated to deal with empty predictions (when some classes can't be predicted)\n","5/10 10:35 - Updated formatting to be numeric\n","36/9 13:02 - Finished basic implementation\n","'''\n","\n","# Scoring function Involving Accuracy, Precision and Recall, F1, Log loss\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","\n","def scoring_function(y_pred_df, y_test_df, all_classes=None):\n","    '''\n","    Calculates various performance metrics for the predictions.\n","\n","    Parameters:\n","    y_pred_df: DataFrame containing predictions.\n","    y_test_df: DataFrame containing true labels.\n","    all_classes: List of all possible classes.\n","\n","    Returns:\n","    metrics: Dictionary containing accuracy, precision, recall, f1 score, and log loss.\n","    '''\n","\n","    # Merge the prediction and actual outcome dataframes using Animal ID\n","    df_combined = pd.merge(y_pred_df, y_test_df, on='Animal.ID')\n","\n","    if df_combined.empty:\n","        print(\"No data in the combined DataFrame.\")\n","        return None\n","\n","    # Convert the predicted probabilities into predicted classes (argmax)\n","    y_pred_class = df_combined.iloc[:, 1:-1].idxmax(axis=1).astype(int)\n","    y_true = df_combined['Outcome.Type'].astype(int)\n","\n","    # If all_classes is None, use unique classes from y_true\n","    if all_classes is None:\n","        all_classes = np.unique(y_true)  # Get unique classes from y_true\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_true, y_pred_class)\n","\n","    # Calculate metrics only for predicted classes\n","    unique_pred_classes = np.unique(y_pred_class)\n","    precision = precision_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    recall = recall_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","    f1 = f1_score(y_true, y_pred_class, average='weighted', zero_division=0, labels=unique_pred_classes)\n","\n","    # Ensure that y_pred_probs contains probabilities for all classes\n","    y_pred_probs = df_combined.iloc[:, 1:-1].values\n","\n","    # Create an array to ensure all classes are represented in predictions\n","    complete_probs = np.zeros((y_pred_probs.shape[0], len(all_classes)))  # Initialize with zeros\n","    for i, class_label in enumerate(all_classes):\n","        if class_label in df_combined.columns:\n","            complete_probs[:, i] = y_pred_probs[:, class_label - 1]  # Fill with actual probabilities\n","\n","    # Calculate log loss with all classes present in the labels\n","    logloss = log_loss(y_true, complete_probs, labels=all_classes)\n","\n","    # Print the metrics\n","    print(f\"\\nAccuracy: {accuracy:.4f}\")\n","    print(f\"Precision: {precision:.4f}\")\n","    print(f\"Recall: {recall:.4f}\")\n","    print(f\"F1 Score: {f1:.4f}\")\n","    print(f\"Log Loss: {logloss:.4f}\")\n","\n","    # Return the metrics as a dictionary\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1_score': f1,\n","        'log_loss': logloss\n","    }\n","\n","  '''\n","  y_pred_df :\n","  Animal ID   1 2 .... 15\n","  101010      0.1 0.2.... 0.0\n","\n","  y_true_df:\n","  Animal ID   Outcome\n","  10129109    2\n","  '''\n","\n","\n","\n","# # Testing\n","\n","# # Example mappings\n","# outcome_type_mapping = {\n","#     'Return to Owner': 1, 'Transfer': 2, 'Adoption': 3, 'Euthanasia': 4,\n","#     'Died': 5, 'Rto-Adopt': 6, 'Disposal': 7, 'Missing': 8,\n","#     'Stolen': 9, 'Relocate': 10, 'Lost': 11, 'Foster': 12,\n","#     'Reclaimed': 13, 'Escaped': 14, 'Released To Wild': 15\n","# }\n","\n","# # Inverse mapping for easier reference later\n","# inv_outcome_type_mapping = {v: k for k, v in outcome_type_mapping.items()}\n","\n","# # Testing variables\n","# train_id = train_id.head(100000)\n","# x_train = x_train.head(100000)\n","# y_train = y_train.head(100000)\n","\n","# test_id = test_id.head(10)\n","# x_test = x_test.head(10)\n","# y_test = y_test.head(10)\n","\n","# # Test Scoring Function\n","\n","# from sklearn.ensemble import RandomForestClassifier\n","# import pandas as pd\n","\n","# # Initialize Random Forest model\n","# rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# # Fit the model\n","# rf_model.fit(x_train, y_train)\n","\n","# # Get predicted probabilities\n","# y_pred_probs = rf_model.predict_proba(x_test)\n","\n","# # Print the shape of the predicted probabilities\n","# print(\"Predicted probabilities shape:\", y_pred_probs.shape)\n","\n","# # Create a DataFrame initialized to zero for all possible classes (1 to 15)\n","# all_classes = list(range(1, 16))\n","# predicted_df = pd.DataFrame(0.0, index=range(len(y_pred_probs)), columns=[inv_outcome_type_mapping[i] for i in all_classes])\n","\n","# # Fill in the predicted probabilities where applicable\n","# for idx, class_probs in enumerate(y_pred_probs):\n","#     for class_idx in range(len(class_probs)):\n","#         class_label = class_idx + 1  # Adjust for 1-based class labels\n","#         predicted_df.at[idx, inv_outcome_type_mapping[class_label]] = class_probs[class_idx]\n","\n","# # Add Animal IDs as the first column in the predicted DataFrame\n","# predicted_df.insert(0, 'Animal.ID', test_id.reset_index(drop=True))\n","\n","# # Combine actual outcomes with Animal IDs\n","# actual_df = pd.DataFrame({\n","#     'Animal.ID': test_id.reset_index(drop=True),\n","#     'Outcome.Type': y_test.reset_index(drop=True).map(inv_outcome_type_mapping)\n","# })\n","\n","# # Display both DataFrames for comparison\n","# print(\"Predicted Probabilities with Animal IDs:\")\n","# print(predicted_df.head())\n","\n","# print(\"\\nActual Outcomes with Animal IDs:\")\n","# print(actual_df.head())\n","\n","# # Prediction conversion from probability to predicted class labels (for debugging)\n","# # Calculate predicted class labels based on the highest probability\n","# predicted_classes = y_pred_probs.argmax(axis=1) + 1  # Get the index of the max probability, adjusting for 1-based class labels\n","\n","# # Map the predicted classes to their corresponding outcome types\n","# predicted_labels = [inv_outcome_type_mapping[cls] for cls in predicted_classes]\n","\n","# # Create a new DataFrame for predicted outcomes\n","# predicted_outcomes_df = pd.DataFrame({\n","#     'Animal.ID': test_id.reset_index(drop=True),\n","#     'Predicted.Outcome.Type': predicted_labels\n","# })\n","\n","# # Display the predicted outcomes DataFrame\n","# print(\"Predicted Outcomes with Animal IDs:\")\n","# print(predicted_outcomes_df.head())\n","\n","# # Calculate and display the metrics\n","# metrics = scoring_function(predicted_df, actual_df)\n"],"metadata":{"id":"0rgFBDPKlzso"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Logging\n","29/7/24 12:30 - Done basic implementation for PCA\n","'''\n","\n","# PCA function\n","\n","# Remember to strip off Animal ID before doing so\n","\n","from sklearn.decomposition import PCA\n","import pandas as pd\n","\n","def pca_combined(x_train, x_test, n_components=50):\n","    \"\"\"\n","    Apply PCA on the combined x_train and x_test datasets to ensure consistent components.\n","\n","    Parameters:\n","    - x_train: The training dataset (must be numerical).\n","    - x_test: The testing dataset (must be numerical).\n","    - n_components: The number of principal components to keep.\n","\n","    Returns:\n","    - x_train_pca: PCA-transformed training data.\n","    - x_test_pca: PCA-transformed testing data.\n","    \"\"\"\n","    # Combine the training and test datasets\n","    combined_data = pd.concat([x_train, x_test], axis=0)\n","\n","    # Fit PCA on the combined data\n","    pca = PCA(n_components=n_components, random_state=42)\n","    combined_pca = pca.fit_transform(combined_data)\n","\n","    # Split the transformed data back into train and test sets\n","    x_train_pca = combined_pca[:x_train.shape[0], :]\n","    x_test_pca = combined_pca[x_train.shape[0]:, :]\n","\n","    # Convert to DataFrames with appropriate column names\n","    x_train_pca_df = pd.DataFrame(x_train_pca, columns=[f'pca_{i}' for i in range(n_components)])\n","    x_test_pca_df = pd.DataFrame(x_test_pca, columns=[f'pca_{i}' for i in range(n_components)])\n","\n","    return x_train_pca_df, x_test_pca_df\n","\n","\n","# # Testing\n","\n","# x_train_pca = x_train.head(100)\n","# x_test_pca = x_test.head(100)\n","\n","# # Apply PCA on the combined x_train and x_test to ensure consistency\n","# x_train_pca, x_test_pca = pca_combined(x_train_pca, x_test_pca, n_components=50)\n","\n","# # Sample Output for viewing\n","# x_train_pca.to_csv('x_train_pca.csv', index=False)\n","# x_test_pca.to_csv('x_test_pca.csv', index=False)\n","\n","# Just use it as your new x train and x test for prediction"],"metadata":{"id":"3RtiEyWBkDTX"},"execution_count":null,"outputs":[]}]}