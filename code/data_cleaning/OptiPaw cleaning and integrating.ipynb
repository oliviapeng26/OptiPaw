{"cells":[{"cell_type":"markdown","metadata":{"id":"vE56j5m9iH3U"},"source":["**Austin data set cleaning and merging:**\n","1. merged austin_intakes and austin_outcomes by Animal ID, and discarded any rows that don't have matching Animal IDs -> austin (merged df)\n","\n","3. remove \"*\" in the Name_x and Name_y entries\n","\n","2. finding mismatches between col_x and col_y of austin\n","- MonthYear mismatches + inspecting DateTime: means time of intake or outcome\n","- deleting duplicate columns that have no mismatches (Name_y, AnimalType_y, Breed_y, Color_y)\n","- renamed x columns to without \"_x\"\n","\n","3. checking and dropping duplicate rows (110 rows)\n","\n","4. removing unwanted attributes (Outcome Subtype)\n","\n","About rows with missing values...\n","- rows with missing values removed from visualization data set\n","- need further investigation for ML dataset (accuracy, ROC AUC score)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2202,"status":"ok","timestamp":1727306989115,"user":{"displayName":"Yitong Yao","userId":"14266407740810554554"},"user_tz":-600},"id":"8-8VD9c1YB4C","outputId":"176b0250-de1f-4857-d585-1b189fe76f17"},"outputs":[{"output_type":"stream","name":"stdout","text":["(167243, 12)\n","(167207, 12)\n","  Animal ID        Name_x              DateTime_x   MonthYear_x  \\\n","0   A786884        *Brock  01/03/2019 04:19:00 PM  January 2019   \n","1   A706918         Belle  07/05/2015 12:59:00 PM     July 2015   \n","2   A724273       Runster  04/14/2016 06:43:00 PM    April 2016   \n","3   A665644           NaN  10/21/2013 07:59:00 AM  October 2013   \n","4   A857105  Johnny Ringo  05/12/2022 12:23:00 AM      May 2022   \n","\n","                        Found Location    Intake Type Intake Condition  \\\n","0  2501 Magin Meadow Dr in Austin (TX)          Stray           Normal   \n","1     9409 Bluegrass Dr in Austin (TX)          Stray           Normal   \n","2   2818 Palomino Trail in Austin (TX)          Stray           Normal   \n","3                          Austin (TX)          Stray             Sick   \n","4   4404 Sarasota Drive in Austin (TX)  Public Assist           Normal   \n","\n","  Animal Type_x Sex upon Intake Age upon Intake  ...              DateTime_y  \\\n","0           Dog   Neutered Male         2 years  ...  01/08/2019 03:11:00 PM   \n","1           Dog   Spayed Female         8 years  ...  07/05/2015 03:13:00 PM   \n","2           Dog     Intact Male       11 months  ...  04/21/2016 05:17:00 PM   \n","3           Cat   Intact Female         4 weeks  ...  10/21/2013 11:39:00 AM   \n","4           Cat   Neutered Male         2 years  ...  05/12/2022 02:35:00 PM   \n","\n","  MonthYear_y Date of Birth     Outcome Type Outcome Subtype Animal Type_y  \\\n","0    Jan 2019    01/03/2017         Transfer         Partner           Dog   \n","1    Jul 2015    07/05/2007  Return to Owner             NaN           Dog   \n","2    Apr 2016    04/17/2015  Return to Owner             NaN           Dog   \n","3    Oct 2013    09/21/2013         Transfer         Partner           Cat   \n","4    May 2022    05/12/2020         Transfer         Partner           Cat   \n","\n","  Sex upon Outcome Age upon Outcome                   Breed_y       Color_y  \n","0    Neutered Male          2 years                Beagle Mix      Tricolor  \n","1    Spayed Female          8 years  English Springer Spaniel   White/Liver  \n","2    Neutered Male           1 year               Basenji Mix   Sable/White  \n","3    Intact Female          4 weeks    Domestic Shorthair Mix        Calico  \n","4    Neutered Male          2 years        Domestic Shorthair  Orange Tabby  \n","\n","[5 rows x 23 columns]\n","(213283, 23)\n"]}],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","\n","austin_intakes = pd.read_csv('https://query.data.world/s/untanflczmb6zmswifydyqtg4mf6db?dws=00000')\n","austin_outcomes = pd.read_csv('https://query.data.world/s/jetxuufbm75thutkjc3ryrtmzaau36?dws=00000')\n","# austin_intakes = pd.read_csv(\"Austin Animal Center Intakes.csv\")\n","# austin_outcomes = pd.read_csv(\"Austin Animal Center Outcomes.csv\")\n","\n","print(austin_intakes.shape)\n","print(austin_outcomes.shape)\n","\n","#merging 2 data sets based on Animal ID and inner join that discards any rows that don't have matching Animal IDs\n","austin = pd.merge(austin_intakes, austin_outcomes, on = \"Animal ID\", how=\"inner\")\n","print(austin.head())\n","print(austin.shape) #less rows than the individual rows combined meaning some rows were discarded due to non-matching Animal IDs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTpqBtN7lhr_"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fCBpUV66we7Z"},"outputs":[],"source":["#remove \"*\" in Name_x and Name_y\n","austin['Name_x'] = austin['Name_x'].str.replace('*', '', regex=False)\n","austin['Name_y'] = austin['Name_y'].str.replace('*', '', regex=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9IgF0rEedXxN"},"outputs":[],"source":["# print(austin_intakes.columns)\n","# print(austin_outcomes.columns)\n","# print(austin.columns)\n","\n","#function that finds mismatches between 2 columns (not counting NaN since NaN are auto mismatches)\n","def find_mismatches(df, col_x, col_y):\n","  mismatches_not_nan = df[(df[col_x] != df[col_y]) & (~df[col_x].isna()) & (~df[col_y].isna())]\n","  return mismatches_not_nan[[col_x, col_y]]\n","\n","print(find_mismatches(austin, \"Name_x\", \"Name_y\"))\n","print(find_mismatches(austin, \"MonthYear_x\", \"MonthYear_y\")) #diff formats -> further checks\n","print(find_mismatches(austin, \"Animal Type_x\", \"Animal Type_y\"))\n","print(find_mismatches(austin, \"Breed_x\", \"Breed_y\"))\n","print(find_mismatches(austin, \"Color_x\", \"Color_y\"))\n","\n","#further checking MonthYear: check if the 3 letters in MonthYear_y is in MonthYear_x, or year in MonthYear_y = MonthYear_x\n","mismatches_custom = austin[(austin['MonthYear_x'].str[:3].str.lower() != austin['MonthYear_y'].str[:3].str.lower()) | (austin['MonthYear_x'].str[-4:] != austin['MonthYear_y'].str[-4:])]\n","print(mismatches_custom[['MonthYear_x', 'MonthYear_y']])\n","\n","#renaming MonthYear_x to MonthYear_intake, MonthYear_y to MonthYear_outcome (same applies to DateTime)\n","austin = austin.rename(columns={\n","    'MonthYear_x': 'MonthYear_intake',\n","    'MonthYear_y': 'MonthYear_outcome',\n","    'DateTime_x': 'DateTime_intake',\n","    'DateTime_y': 'DateTime_outcome'\n","})\n","\n","#dropping columns with no mismatches (name_y, animaltype_y, breed_y, color_y)\n","austin = austin.drop(austin.filter(like='_y').columns, axis=1)\n","\n","#renaming \"_x\" columns\n","austin.columns = austin.columns.str.replace(\"_x\", \"\")\n","print(austin.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7F2fw9hqkoj"},"outputs":[],"source":["#checking and dropping duplicate rows\n","print(austin.duplicated().sum())\n","austin = austin.drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mr6jNxbosOG6"},"outputs":[],"source":["#removing unwanted attribute (Outcome Subtype)\n","austin = austin.drop('Outcome Subtype', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oBmytwN-v3As"},"outputs":[],"source":["#before removing missing values, write file for ML\n","austin.to_csv(\"/content/austin_ML.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJ7VbkXYvcMo"},"outputs":[],"source":["#checking missing values per column\n","missing_values = austin.isna().sum()\n","print(missing_values)\n","\n","#removing all rows with missing values\n","austin_cleaned = austin.dropna()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"731x8xxbxCBL"},"outputs":[],"source":["#write cleaned file for data visualization\n","austin_cleaned.to_csv(\"/content/austin_VIS.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"F3KBONQHyHcy"},"source":["All data set merging and cleaning:\n","1. attributes not in all data sets removed\n","\n","2. combined similar attributes together\n","  - Sex upon outcome chosen over Sex upon intake to merge with Sex (higher feature importance score, higher correlation with Outcome Type)\n","  - Age upon outcome chosen over Age upon intake to merge with animalage (consistency with Sex upon outcome)\n","\n","About rows with missing values...\n","- rows with missing values removed from visualization data set\n","- need further investigation for ML dataset (accuracy, ROC AUC score)"]},{"cell_type":"code","source":["austin_cleaned.columns"],"metadata":{"id":"MQ0qSInOv99M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["austin_cleaned['Outcome Type'].unique()"],"metadata":{"id":"P5ath3tSwFoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Assuming `austin_cleaned` is your DataFrame\n","outcomes = austin_cleaned.groupby(['Animal Type', 'Outcome Type']).size().reset_index(name='N')\n","\n","# Pivot the data to get the proportions\n","outcomes_pivot = outcomes.pivot(index='Animal Type', columns='Outcome Type', values='N').fillna(0)\n","outcomes_pivot = outcomes_pivot.div(outcomes_pivot.sum(axis=1), axis=0)  # Normalize to get proportions\n","\n","# Plotting the stacked bar chart\n","outcomes_pivot.plot(kind='bar', stacked=True, color  = [\n","    '#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231',\n","    '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080'\n","],edgecolor='black')\n","\n","plt.xlabel('Animal')\n","plt.ylabel('Proportion')\n","plt.title('Outcomes')\n","plt.legend(title='OutcomeType', bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"aOq_r6OBwNnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rASkBrm_yTKs"},"outputs":[],"source":["#opening all data sets\n","california = pd.read_csv(\"california-animal-data.csv\")\n","indiana = pd.read_csv(\"indiana-animal-data.csv\")\n","austin = pd.read_csv(\"austin_VIS.csv\")\n","\n","#adding a state attribute to identify which state each pet came from\n","austin[\"State\"] = \"Austin\"\n","california[\"State\"] = \"California\"\n","indiana[\"State\"] = \"Indiana\"\n","\n","#creating integrated data set\n","optipaw = pd.concat([austin, california, indiana], ignore_index=True)\n","\n","#dropping irrelevant columns, columns not in all data sets\n","optipaw = optipaw.drop(['sheltercode',\n","                        'identichipnumber',\n","                        'Outcome Subtype',\n","                        'Intake Subtype',\n","                        'intake_is_dead',\n","                        'outcome_is_dead',\n","                        'outcome_is_dead',\n","                        'was_outcome_alive',\n","                        'istransfer',\n","                        'istrial',\n","                        'isdoa',\n","                        'Intake Condition',\n","                        'latitude',\n","                        'longitude', 'Crossing',\n","                        'Jurisdiction',\n","                        'Secondary Color',\n","                        'geopoint',\n","                        'MonthYear_intake',\n","                        'MonthYear_outcome', 'deceaseddate',\n","                        'deceasedreason',\n","                        'diedoffshelter',\n","                        'puttosleep',\n","                        'returndate',\n","                        'returnedreason',\n","                        'Sex upon Intake',\n","                        'Age upon Intake'], axis=1)\n","\n","#ensuring no duplicate rows, remove if found\n","print(optipaw.duplicated().sum())\n","optipaw = optipaw.drop_duplicates()\n","\n","#attributes of 3 data sets\n","print(california.columns)\n","print(indiana.columns)\n","print(austin.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4OkiyFAt5Cvv"},"outputs":[],"source":["#combining \"Animal ID\" and \"id\" into \"Animal ID\"\n","optipaw['Animal ID'] = optipaw['Animal ID'].fillna(optipaw['id'])\n","optipaw = optipaw.drop(['id'], axis=1)\n","\n","#combining \"Name\", \"Animal Name\" and \"animalname\" into \"Name\"\n","optipaw['Name'] = optipaw['Name'].fillna(optipaw['Animal Name']).fillna(optipaw['animalname'])\n","optipaw = optipaw.drop(['Animal Name', 'animalname'], axis=1)\n","\n","#combining \"Animal Type\" and \"speciesname\" into \"Animal Type\"\n","optipaw['Animal Type'] = optipaw['Animal Type'].fillna(optipaw['speciesname'])\n","optipaw = optipaw.drop(['speciesname'], axis=1)\n","\n","#combining \"Date of Birth\" and \"DOB\" into \"DOB\"\n","optipaw['DOB'] = optipaw['DOB'].fillna(optipaw['Date of Birth'])\n","optipaw = optipaw.drop(['Date of Birth'], axis=1)\n","\n","#combining 'Primary Color' and 'basecolour' into 'Color'\n","optipaw['Color'] = optipaw['Color'].fillna(optipaw['basecolour']).fillna(optipaw['Primary Color'])\n","optipaw = optipaw.drop(['Primary Color', 'basecolour'], axis=1)\n","\n","#combining 'Sex upon Outcome', 'Sex' and 'sexname' into 'Sex' - 'Sex upon Outcome' has higher importance and correlation score with 'Outcome Type'\n","optipaw.loc[:, 'Sex'] = optipaw['Sex upon Outcome'].fillna(optipaw['Sex']).fillna(optipaw['sexname'])\n","optipaw = optipaw.drop(['Sex upon Outcome', 'sexname'], axis=1)\n","\n","#combining 'Age upon Outcome' and 'animalage into 'Age' - 'Age upon Outcome' consistent with 'Sex upon Outcome' though importance and correlation scores are contradictory\n","optipaw.loc[:, 'Age'] = optipaw['Age upon Outcome'].fillna(optipaw['animalage'])\n","optipaw = optipaw.drop(['Age upon Outcome', 'animalage'], axis=1)\n","\n","#combining 'Breed' and 'breedname' into 'Breed'\n","optipaw['Breed'] = optipaw['Breed'].fillna(optipaw['breedname'])\n","optipaw = optipaw.drop(['breedname'], axis=1)\n","\n","#combining \"Intake Type\", \"Reason for Intake\", \"intakereason\" into \"Intake Reason\"\n","optipaw['Intake Reason'] = optipaw['Intake Type'].fillna(optipaw['Reason for Intake']).fillna(optipaw['intakereason'])\n","optipaw = optipaw.drop(['Intake Type', 'Reason for Intake', 'intakereason'], axis=1)\n","\n","#change all to YYYY-MM-DD format and combining 'DateTime_intake', 'Intake Date', 'intakedate' into 'Intake Date'\n","optipaw['DateTime_intake'] = pd.to_datetime(optipaw['DateTime_intake'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['Intake Date'] = pd.to_datetime(optipaw['Intake Date'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['intakedate'] = pd.to_datetime(optipaw['intakedate'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['Intake Date'] = optipaw['DateTime_intake'].fillna(optipaw['Intake Date']).fillna(optipaw['intakedate'])\n","optipaw = optipaw.drop(['DateTime_intake', 'intakedate'], axis=1)\n","\n","#change all to YYYY-MM-DD format and combining 'DateTime_outcome', 'Outcome Date', 'movementdate' into 'Outcome Date'\n","optipaw['DateTime_outcome'] = pd.to_datetime(optipaw['DateTime_outcome'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['Outcome Date'] = pd.to_datetime(optipaw['Outcome Date'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['movementdate'] = pd.to_datetime(optipaw['movementdate'], format='%m/%d/%Y %I:%M:%S %p', errors='coerce').dt.strftime('%Y-%m-%d')\n","optipaw['Outcome Date'] = optipaw['DateTime_outcome'].fillna(optipaw['Outcome Date']).fillna(optipaw['movementdate'])\n","optipaw = optipaw.drop(['DateTime_outcome', 'movementdate'], axis=1)\n","\n","#combining 'movementtype' and 'Outcome Type' into 'Outcome Type'\n","optipaw['Outcome Type'] = optipaw['Outcome Type'].fillna(optipaw['movementtype'])\n","optipaw = optipaw.drop(['movementtype'], axis=1)\n","\n","#combining 'Found Location' and 'location' into 'Found Location'\n","optipaw['Found Location'] = optipaw['Found Location'].fillna(optipaw['location'])\n","optipaw = optipaw.drop(['location'], axis=1)\n","\n","optipaw.columns\n"]},{"cell_type":"code","source":["optipaw['Name'] = optipaw['Name'].str.title()\n","optipaw['Animal Type'] = optipaw['Animal Type'].str.title()\n","optipaw['Breed'] = optipaw['Breed'].str.title()\n","optipaw['Color'] = optipaw['Color'].str.title()\n","optipaw['Outcome Type'] = optipaw['Outcome Type'].str.title()\n","optipaw['Sex'] = optipaw['Sex'].str.title()\n","optipaw['Intake Reason'] = optipaw['Intake Reason'].str.title()"],"metadata":{"id":"8oPuPyFld3Rg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N8WSfUaDHaO5"},"outputs":[],"source":["#before dropping missing values, write file for ML\n","optipaw.to_csv(\"/content/optipaw_ML.csv\", index=False)"]},{"cell_type":"code","source":["missing_values = optipaw.isna().sum()\n","\n","print(missing_values)\n","\n","# Get the total number of rows\n","total_rows = len(optipaw)\n","\n","# Calculate missing values as proportion of total rows\n","missing_values_proportion = optipaw.isna().sum() / total_rows\n","\n","# Print the result\n","print(missing_values_proportion)\n","\n","print(optipaw.shape)\n","\n","\n","import matplotlib.pyplot as plt\n","from upsetplot import UpSet, from_indicators\n","\n","# Create a boolean DataFrame for missing values\n","missing_values = optipaw.isna()\n","\n","# Generate an UpSet plot based on missing data\n","upset_data = from_indicators(missing_values)\n","\n","# Plot the UpSet chart\n","UpSet(upset_data).plot()\n","\n","# Show the plot\n","plt.show()\n","\n","\n","optipaw_cleaned = optipaw.dropna()\n","optipaw_cleaned = optipaw_cleaned[optipaw_cleaned['Sex'] != 'Unknown']\n","\n","optipaw_cleaned.to_csv(\"/content/optipaw_VIS.csv\", index=False)\n","\n","print(optipaw_cleaned.shape) #less than half of the rows were deleted"],"metadata":{"id":"OHpS2zOdwFbx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optipaw_mistake = pd.read_csv(\"optipaw.csv\")\n","missing_values_mistakes = optipaw_mistake.isna().sum()\n","print(missing_values_mistakes)"],"metadata":{"id":"660mokW0p8DB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FA-57Lwrl8n3"},"outputs":[],"source":["# import pandas as pd\n","# import numpy as np\n","# import re\n","# from sklearn.impute import SimpleImputer\n","\n","\n","# missing_val_dataset = pd.read_csv('austin_VIS.csv')\n","# val_dataset = pd.read_csv('austin_ML.csv')\n","\n","# # display(missing_val_dataset.head())\n","# # display(val_dataset.head())\n","\n","\n","# # EDA for machine learning\n","\n","\n","# def preprocess(df):\n","#     # Remove A for the Animal ID, and give 0 if invalid\n","#     df['Animal ID'] = df['Animal ID'].apply(lambda x: 1 if isinstance(x, str) else 0)\n","\n","#     # 1 if it is a valid string, else make it 0\n","#     df['Name'] = df['Name'].apply(lambda x: 1 if isinstance(x, str) else 0)\n","\n","#     # Fix DateTime_Intake\n","#     def classify_datetime(value):\n","#         if isinstance(value, str):\n","#             if re.match(r'^\\d{5,}\\.\\d+$', value):\n","#                 return 1\n","#             elif re.match(r'^\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2} (AM|PM)$', value):\n","#                 return 2\n","#         return 0\n","\n","#     df['DateTime_intake'] = df['DateTime_intake'].apply(classify_datetime)\n","\n","#     # Fix MonthYear_intake\n","\n","\n","#     df['MonthYear_intake'] = df['MonthYear_intake'].apply(lambda x: 1 if isinstance(x, (int, float)) and not pd.isna(x) else 0)\n","\n","#     # Found Location 1 if string, else 0\n","#     df['Found Location'] = df['Found Location'].apply(lambda x: 1 if isinstance(x, str) else 0)\n","\n","#     # Intake Type mapping\n","#     intake_type_map = {\n","#         'Stray': 1,\n","#         'Public Assist': 2,\n","#         'Owner Surrender': 3,\n","#         'Abandoned': 4,\n","#         'Wildlife': 5,\n","#         'Euthanasia Request': 6\n","#     }\n","\n","#     df['Intake Type'] = df['Intake Type'].map(intake_type_map).fillna(0).astype(int)\n","\n","#     # Intake Condition mapping\n","#     intake_condition_map = {\n","#         'Normal': 1,\n","#         'Sick': 2,\n","#         'Injured': 3,\n","#         'Pregnant': 4,\n","#         'Neonatal': 5,\n","#         'Nursing': 6,\n","#         'Aged': 7,\n","#         'Medical': 8,\n","#         'Unknown': 9,\n","#         'Med Attn': 10,\n","#         'Other': 11,\n","#         'Behavior': 12,\n","#         'Feral': 13,\n","#         'Med Urgent': 14,\n","#         'Parvo': 15,\n","#         'Space': 16,\n","#         'Agonal': 17,\n","#         'Neurologic': 18,\n","#         'Panleuk': 19,\n","#         'Congenital': 20\n","#     }\n","\n","#     df['Intake Condition'] = df['Intake Condition'].map(intake_condition_map).fillna(0).astype(int)\n","\n","#     # Animal Type mapping\n","#     animal_type_map = {\n","#         'Dog': 1,\n","#         'Cat': 2,\n","#         'Other': 3,\n","#         'Bird': 4,\n","#         'Livestock': 5\n","#     }\n","\n","#     df['Animal Type'] = df['Animal Type'].map(animal_type_map).fillna(0).astype(int)\n","\n","#     # Sex upon Intake mapping\n","#     sex_upon_intake_map = {\n","#         'Neutered Male': 1,\n","#         'Spayed Female': 2,\n","#         'Intact Male': 3,\n","#         'Intact Female': 4,\n","#         'Unknown': 5\n","#     }\n","\n","#     df['Sex upon Intake'] = df['Sex upon Intake'].map(sex_upon_intake_map).fillna(0).astype(int)\n","\n","#     # Age to days\n","#     def age_to_days(age_str):\n","#         if isinstance(age_str, str):\n","#             age_str = age_str.lower().replace(' ', '')\n","#             years = months = days = 0\n","#             if 'year' in age_str:\n","#                 years = int(age_str.split('year')[0].strip().replace('s', ''))\n","#             if 'month' in age_str:\n","#                 months = int(age_str.split('month')[0].split()[-1].strip())\n","#             if 'day' in age_str:\n","#                 days = int(age_str.split('day')[0].split()[-1].strip())\n","#             total_days = years * 365 + months * 30 + days\n","#             return abs(total_days)\n","#         return 0\n","\n","#     df['Age upon Intake'] = df['Age upon Intake'].apply(age_to_days)\n","\n","#     # Breed binary\n","#     df['Breed'] = df['Breed'].apply(lambda x: 1 if isinstance(x, str) else 0)\n","\n","#     # Color binary\n","#     df['Color'] = df['Color'].apply(lambda x: 1 if isinstance(x, str) else 0)\n","\n","#     # DateTime_outcome\n","#     df['DateTime_outcome'] = df['DateTime_outcome'].apply(classify_datetime)\n","\n","#     # MonthYear_outcome\n","#     df['MonthYear_outcome'] = df['MonthYear_outcome'].apply(lambda x: 1 if isinstance(x, (int, float)) and not pd.isna(x) else 0)\n","\n","#     # Date of Birth classification\n","#     def classify_dob(value):\n","#         if isinstance(value, int):\n","#             return 1\n","#         elif isinstance(value, str) and re.match(r'^\\d{2}/\\d{2}/\\d{4}$', value):\n","#             return 2\n","#         return 0\n","\n","#     df['Date of Birth'] = df['Date of Birth'].apply(classify_dob)\n","\n","#     # Outcome Type mapping\n","#     outcome_type_map = {\n","#         'Transfer': 1,\n","#         'Return to Owner': 2,\n","#         'Adoption': 3,\n","#         'Euthanasia': 4,\n","#         'Disposal': 5,\n","#         'Died': 6,\n","#         'Rto-Adopt': 7,\n","#         'Missing': 8,\n","#         '0': 9,\n","#         'Relocate': 10,\n","#         'Lost': 11,\n","#         'Stolen': 12\n","#     }\n","\n","#     df['Outcome Type'] = df['Outcome Type'].map(outcome_type_map).fillna(0).astype(int)\n","\n","#     # Sex upon Outcome mapping\n","#     sex_upon_outcome_map = {\n","#         'Neutered Male': 1,\n","#         'Spayed Female': 2,\n","#         'Intact Female': 3,\n","#         'Unknown': 4,\n","#         'Intact Male': 5\n","#     }\n","\n","#     df['Sex upon Outcome'] = df['Sex upon Outcome'].map(sex_upon_outcome_map).fillna(0).astype(int)\n","\n","#     # Age upon Outcome\n","#     df['Age upon Outcome'] = df['Age upon Outcome'].apply(age_to_days)\n","\n","#     # Impute missing values\n","#     numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n","#     categorical_cols = df.select_dtypes(include=['object']).columns\n","\n","#     numeric_imputer = SimpleImputer(strategy='median')\n","#     df[numeric_cols] = numeric_imputer.fit_transform(df[numeric_cols])\n","\n","#     categorical_imputer = SimpleImputer(strategy='most_frequent')\n","#     #df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n","\n","#     return df\n","\n","# # Process datasets\n","# cleaned_missing = preprocess(missing_val_dataset)\n","# cleaned_ok = preprocess(val_dataset)\n","\n","# # Output to CSV files\n","# #cleaned_missing.to_csv('missing_file.csv', index=False)\n","# #cleaned_ok.to_csv('normal_file.csv', index=False)\n","\n","\n","# a = cleaned_missing\n","# x_train = a.drop(columns=a.columns[15])  # Features\n","# y_train = a.iloc[:, 15]  # Labels - outcome type\n","\n","# b = cleaned_ok\n","# x1_train = b.drop(columns=a.columns[15])  # Features\n","# y1_train = b.iloc[:, 15]  # Labels - outcome type\n","\n","# # taking first 10k value\n","# x_train = x_train.head(10000)\n","# y_train = y_train.head(10000)\n","\n","# x1_train = x1_train.head(10000)\n","# y1_train = y1_train.head(10000)\n","\n","\n","# print(x_train.head())\n","# print(y_train.head())\n","\n","# print(x1_train.head())\n","# print(y1_train.head())\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"03bbihDD6Ado"},"outputs":[],"source":["# # Random forrest test with accuracy score and ROC\n","\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.metrics import roc_auc_score, accuracy_score\n","# from sklearn.preprocessing import label_binarize\n","# import numpy as np\n","\n","# def evaluate_random_forest(X_train, y_train):\n","#     # Initialize RandomForestClassifier with fewer estimators and max_depth\n","#     rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n","\n","#     # Fit the model\n","#     rf.fit(X_train, y_train)\n","\n","#     # Predict probabilities\n","#     y_train_proba = rf.predict_proba(X_train)\n","\n","#     # Predict labels\n","#     y_train_pred = rf.predict(X_train)\n","\n","#     # Calculate ROC AUC score for multi-class classification\n","#     # Binarize the output labels\n","#     y_train_bin = label_binarize(y_train, classes=np.unique(y_train))\n","\n","#     # Calculate ROC AUC score\n","#     roc_auc = roc_auc_score(y_train_bin, y_train_proba, average='macro', multi_class='ovr')\n","\n","#     # Calculate Accuracy\n","#     accuracy = accuracy_score(y_train, y_train_pred)\n","\n","#     return accuracy, roc_auc\n","\n","# accuracy, roc_auc = evaluate_random_forest(x_train, y_train)\n","# print(\"Cleaned Missing:\")\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n","\n","# accuracy, roc_auc = evaluate_random_forest(x1_train, y1_train)\n","# print(\"Cleaned Normal:\")\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iO5a7m4UB5CZ"},"outputs":[],"source":["# # Feature importance score for Sex upon Intake and Sex upon Outcome\n","\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.metrics import accuracy_score, roc_auc_score\n","# import matplotlib.pyplot as plt\n","# import pandas as pd\n","\n","# # Define features of interest\n","# features_of_interest = ['Age upon Intake', 'Age upon Outcome']\n","\n","# # Extract features and labels from the first 10,000 rows\n","# x = x1_train[features_of_interest]  # Select columns by passing a list of column names\n","# y = y1_train  # outcome type\n","\n","# # Ensure y is numeric (if it's not already)\n","# y = pd.to_numeric(y, errors='coerce')\n","\n","# # Train a Random Forest model\n","# rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n","# rf.fit(x, y)\n","\n","# # Get feature importances\n","# importances = rf.feature_importances_\n","\n","# # Create a DataFrame for visualization\n","# importance_df = pd.DataFrame({\n","#     'Feature': features_of_interest,\n","#     'Importance': importances\n","# }).sort_values(by='Importance', ascending=False)\n","\n","# # Print the feature importance scores\n","# print(importance_df)\n","\n","# # Visualize the importances\n","# plt.figure(figsize=(10, 6))\n","# plt.barh(importance_df['Feature'], importance_df['Importance'])\n","# plt.xlabel('Importance')\n","# plt.title('Feature Importance Scores for Selected Features')\n","# plt.gca().invert_yaxis()  # Highest importance at the top\n","# plt.show()\n","\n","# # Evaluate model performance\n","# y_pred = rf.predict(x)  # Use x instead of X\n","# accuracy = accuracy_score(y, y_pred)\n","# roc_auc = roc_auc_score(y, rf.predict_proba(x), multi_class='ovr')  # For multi-class ROC AUC\n","\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n","\n","# # Correlation Score between 'Sex upon Intake' and 'Intake Condition'\n","# correlation_intake = x['Age upon Intake'].corr(y1_train)\n","\n","# # Correlation Score between 'Sex upon Outcome' and 'Intake Condition'\n","# correlation_outcome = x['Age upon Outcome'].corr(y1_train)\n","\n","# print(f\"Correlation score between 'Age upon Intake' and 'Outcome Type': {correlation_intake:.4f}\")\n","# print(f\"Correlation score between 'Age upon Outcome' and 'Outcome Type': {correlation_outcome:.4f}\")\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["# # Feature importance score for Age upon Intake and Age upon Outcome\n","\n","# from sklearn.ensemble import RandomForestClassifier\n","# from sklearn.metrics import accuracy_score, roc_auc_score\n","# import matplotlib.pyplot as plt\n","# import pandas as pd\n","\n","# # Define features of interest\n","# features_of_interest = ['Sex upon Intake', 'Sex upon Outcome']\n","\n","# # Extract features and labels from the first 10,000 rows\n","# x = x1_train[features_of_interest]  # Select columns by passing a list of column names\n","# y = y1_train  # outcome type\n","\n","# # Ensure y is numeric (if it's not already)\n","# y = pd.to_numeric(y, errors='coerce')\n","\n","# # Train a Random Forest model\n","# rf = RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n","# rf.fit(x, y)\n","\n","# # Get feature importances\n","# importances = rf.feature_importances_\n","\n","# # Create a DataFrame for visualization\n","# importance_df = pd.DataFrame({\n","#     'Feature': features_of_interest,\n","#     'Importance': importances\n","# }).sort_values(by='Importance', ascending=False)\n","\n","# # Print the feature importance scores\n","# print(importance_df)\n","\n","# # Visualize the importances\n","# plt.figure(figsize=(10, 6))\n","# plt.barh(importance_df['Feature'], importance_df['Importance'])\n","# plt.xlabel('Importance')\n","# plt.title('Feature Importance Scores for Selected Features')\n","# plt.gca().invert_yaxis()  # Highest importance at the top\n","# plt.show()\n","\n","# # Evaluate model performance\n","# y_pred = rf.predict(x)  # Use x instead of X\n","# accuracy = accuracy_score(y, y_pred)\n","# roc_auc = roc_auc_score(y, rf.predict_proba(x), multi_class='ovr')  # For multi-class ROC AUC\n","\n","# print(f\"Accuracy: {accuracy:.4f}\")\n","# print(f\"ROC AUC Score: {roc_auc:.4f}\")\n","\n","# # Correlation Score between 'Sex upon Intake' and 'Intake Condition'\n","# correlation_intake = x['Sex upon Intake'].corr(y1_train)\n","\n","# # Correlation Score between 'Sex upon Outcome' and 'Intake Condition'\n","# correlation_outcome = x['Sex upon Outcome'].corr(y1_train)\n","\n","# print(f\"Correlation score between 'Sex upon Intake' and 'Outcome Type': {correlation_intake:.4f}\")\n","# print(f\"Correlation score between 'Sex upon Outcome' and 'Outcome Type': {correlation_outcome:.4f}\")\n"],"metadata":{"id":"s8Ke0eXcSufj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Indiana Dataset Feature Scores with Random Forrest\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","indiana_data = pd.read_csv('indiana-animal-data.csv')\n","\n","#indiana preprocess before feeding it to ml\n","def indiana_preprocess(df):\n","  df = df.fillna(0)\n","  # fix intakedate\n","  df['intakedate'] = pd.to_datetime(df['intakedate'])\n","\n","  # Extract hour, day of the week, and month\n","  df['hour'] = df['intakedate'].dt.hour\n","  df['day'] = df['intakedate'].dt.dayofweek  # Monday=0, Sunday=6\n","  df['month'] = df['intakedate'].dt.month\n","\n","  # Normalize time-based features using sine and cosine for cyclic representation\n","  df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n","  df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n","\n","  df['day_sin'] = np.sin(2 * np.pi * df['day'] / 7)\n","  df['day_cos'] = np.cos(2 * np.pi * df['day'] / 7)\n","\n","  df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n","  df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n","\n","  df.drop(['intakedate', 'hour', 'day', 'month'], axis=1, inplace=True)\n","\n","  # fix intakereason\n","\n","  le = LabelEncoder()\n","  df['intakereason'] = le.fit_transform(df['intakereason'].astype(str))\n","\n","  # fix sheltercode\n","  # Remove empty strings and convert to integer\n","  df['sheltercode'] = df['sheltercode'].str[1:].replace('', '0').astype(int)\n","\n","\n","  # fix identitychipnumber, brute force\n","\n","  df['identichipnumber'] = 1\n","\n","\n","  # fix animalname - bruteforce everything to 1\n","  df['animalname'] = 1\n","\n","  # fix breedname\n","  le = LabelEncoder()\n","  df['breedname'] = le.fit_transform(df['breedname'].astype(str))\n","\n","  # label encoder for basecolour\n","  le = LabelEncoder()\n","  df['basecolour'] = le.fit_transform(df['basecolour'].astype(str))\n","\n","  # same goes to species name\n","  le = LabelEncoder()\n","  df['speciesname'] = le.fit_transform(df['speciesname'])\n","\n","  # fix age\n","\n","  # Example transformation for animalage\n","  def convert_age(age_str):\n","      if isinstance(age_str, str):  # Check if the input is a string\n","          parts = age_str[:-1].split(' ')  # Remove the full stop and split\n","          if len(parts) >= 3:  # Ensure there are enough parts\n","              years = int(parts[0])  # Extract years\n","              months = int(parts[2])  # Extract months\n","              return years * 12 + months  # Convert to total months\n","      return 0  # Return 0 or another default value for invalid formats\n","\n","  df['animalage'] = df['animalage'].apply(convert_age)\n","\n","\n","  # label encoder for sexname\n","  le = LabelEncoder()\n","  df['sexname'] = le.fit_transform(df['sexname'].astype(str))\n","\n","  # same for location\n","  le = LabelEncoder()\n","  df['location'] = le.fit_transform(df['location'].astype(str))\n","\n","  # same for movement type\n","  le = LabelEncoder()\n","  df['movementtype'] = le.fit_transform(df['movementtype'].astype(str))\n","\n","  # movement date same as intakedate\n","  df['movementdate'] = pd.to_datetime(df['movementdate'])\n","\n","  # Extract hour, day of the week, and month\n","  df['hour'] = df['movementdate'].dt.hour\n","  df['day'] = df['movementdate'].dt.dayofweek  # Monday=0, Sunday=6\n","  df['month'] = df['movementdate'].dt.month\n","\n","  # Normalize time-based features using sine and cosine for cyclic representation\n","  df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n","  df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n","\n","  df['day_sin'] = np.sin(2 * np.pi * df['day'] / 7)\n","  df['day_cos'] = np.cos(2 * np.pi * df['day'] / 7)\n","\n","  df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n","  df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n","\n","  df.drop(['movementdate', 'hour', 'day', 'month'], axis=1, inplace=True)\n","\n","\n","\n","  # same for returnDate\n","  df['returndate'] = pd.to_datetime(df['returndate'])\n","\n","  # Extract hour, day of the week, and month\n","  df['hour'] = df['returndate'].dt.hour\n","  df['day'] = df['returndate'].dt.dayofweek  # Monday=0, Sunday=6\n","  df['month'] = df['returndate'].dt.month\n","\n","  # Normalize time-based features using sine and cosine for cyclic representation\n","  df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n","  df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n","\n","  df['day_sin'] = np.sin(2 * np.pi * df['day'] / 7)\n","  df['day_cos'] = np.cos(2 * np.pi * df['day'] / 7)\n","\n","  df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n","  df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n","\n","  df.drop(['returndate', 'hour', 'day', 'month'], axis=1, inplace=True)\n","\n","  # label encoder for returnedreason\n","  le = LabelEncoder()\n","  df['returnedreason'] = le.fit_transform(df['returnedreason'].astype(str))\n","\n","  # deceased date\n","  df['deceaseddate'] = pd.to_datetime(df['deceaseddate'])\n","\n","  # Extract hour, day of the week, and month\n","  df['hour'] = df['deceaseddate'].dt.hour\n","  df['day'] = df['deceaseddate'].dt.dayofweek  # Monday=0, Sunday=6\n","  df['month'] = df['deceaseddate'].dt.month\n","\n","  # Normalize time-based features using sine and cosine for cyclic representation\n","  df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n","  df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n","\n","  df['day_sin'] = np.sin(2 * np.pi * df['day'] / 7)\n","  df['day_cos'] = np.cos(2 * np.pi * df['day'] / 7)\n","\n","  df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n","  df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n","\n","\n","  df.drop(['deceaseddate', 'hour', 'day', 'month'], axis=1, inplace=True)\n","\n","  # label encoder for deceasedreason\n","  le = LabelEncoder()\n","  df['deceasedreason'] = le.fit_transform(df['deceasedreason'].astype(str))\n","\n","\n","  return df\n","\n","indiana_data = indiana_preprocess(indiana_data)\n","\n","# imputer\n","def impute_missing_values(df):\n","    for column in df.columns:\n","        if df[column].dtype in ['float64', 'int64']:\n","            df[column].fillna(df[column].mean(), inplace=True)  # Impute with mean\n","        else:\n","            df[column].fillna(df[column].mode()[0], inplace=True)  # Impute with mode\n","    return df\n","\n","indiana_data = impute_missing_values(indiana_data)\n","# print(\"Null Checks\")\n","# print(indiana_data.isnull().sum())\n","# print(\"Infinity Checks\")\n","# print(np.isinf(indiana_data).sum())\n","\n","\n","#indiana_data.to_csv('indiana_processed.csv', index=False)\n","\n","x_train = indiana_data.iloc[:10000].drop(columns=['movementtype'])\n","y_train = indiana_data.iloc[:10000]['movementtype']\n","\n","# model training and featured score\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Build the Random Forest model\n","rf_model = RandomForestClassifier()\n","rf_model.fit(x_train, y_train)\n","\n","# Get feature importances\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","feature_importances = rf_model.feature_importances_\n","feature_names = x_train.columns\n","\n","# Create a bar chart\n","plt.figure(figsize=(10, 6))\n","indices = np.argsort(feature_importances)[::-1]  # Sort feature importances in descending order\n","plt.bar(range(len(feature_importances)), feature_importances[indices], align='center')\n","plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n","plt.title('Feature Indiana Importances from Random Forest')\n","plt.xlabel('Features')\n","plt.ylabel('Importance Score')\n","plt.tight_layout()\n","plt.show()\n","\n","# print(indiana_data['breedname'].value_counts())\n","# print(indiana_data['movementtype'].value_counts())\n","\n","\n","# Calculate the correlation score for breedname and movementtype\n","correlation_score = indiana_data['breedname'].astype('category').cat.codes.corr(indiana_data['movementtype'].astype('category').cat.codes)\n","\n","\n","# Print the correlation score\n","print(\"Correlation score between breedname and movementtype:\", correlation_score)\n","\n"],"metadata":{"id":"U_8H7NX79pkI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merged Austin Dataset Feature Scores with Random Forrest\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","austin_data = pd.read_csv('merged_austin_data.csv')\n","\n","#austin preprocess before feeding it to ml\n","\n","def austin_preprocess(df):\n","  df = df.fillna(0)\n","\n","  # Animal.id.X\n","  df['Animal.ID.x'] = df['Animal.ID.x'].str[1:].astype(int)\n","\n","  # Name.x\n","  le = LabelEncoder()\n","  df['Name.x'] = le.fit_transform(df['Name.x'].astype(str))\n","\n","  #DateTime.x\n","  df['DateTime.x'] = pd.to_datetime(df['DateTime.x'], format='%m/%d/%Y %I:%M:%S %p')\n","\n","  #MonthYear.x\n","  df['MonthYear.x'] = pd.to_datetime(df['MonthYear.x'], format='%B %Y')\n","\n","  # Found.Location\n","  le = LabelEncoder()\n","  df['Found.Location'] = le.fit_transform(df['Found.Location'].astype(str))\n","\n","  # Intake.Type\n","  le = LabelEncoder()\n","  df['Intake.Type'] = le.fit_transform(df['Intake.Type'].astype(str))\n","\n","  # Intake.Condition\n","  le = LabelEncoder()\n","  df['Intake.Condition'] = le.fit_transform(df['Intake.Condition'].astype(str))\n","\n","  # Animal.Type.x\n","  le = LabelEncoder()\n","  df['Animal.Type.x'] = le.fit_transform(df['Animal.Type.x'].astype(str))\n","\n","  # Sex.upon.Intake\n","  le = LabelEncoder()\n","  df['Sex.upon.Intake'] = le.fit_transform(df['Sex.upon.Intake'].astype(str))\n","\n","  # Age.upon.Intake\n","  df['Age.upon.Intake'] = df['Age.upon.Intake'].str.split(' ').str[0].fillna('0').astype(int)\n","\n","  # Breed.x\n","  le = LabelEncoder()\n","  df['Breed.x'] = le.fit_transform(df['Breed.x'].astype(str))\n","\n","  # Color.x\n","  le = LabelEncoder()\n","  df['Color.x'] = le.fit_transform(df['Color.x'].astype(str))\n","\n","  # DateTimeUTC.x\n","  df['DateTimeUTC.x'] = pd.to_datetime(df['DateTimeUTC.x'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","  # Animal.id.y\n","  df['Animal.ID.y'] = df['Animal.ID.y'].str[1:].astype(int)\n","\n","  # Name.y\n","  le = LabelEncoder()\n","  df['Name.y'] = le.fit_transform(df['Name.y'].astype(str))\n","\n","  #DateTime.y\n","  df['DateTime.y'] = pd.to_datetime(df['DateTime.y'], format='%m/%d/%Y %I:%M:%S %p')\n","\n","  #MonthYear.y\n","  # Create a mapping of abbreviated month names to full names\n","  month_map = {\n","      'Jan': 'January',\n","      'Feb': 'February',\n","      'Mar': 'March',\n","      'Apr': 'April',\n","      'May': 'May',\n","      'Jun': 'June',\n","      'Jul': 'July',\n","      'Aug': 'August',\n","      'Sep': 'September',\n","      'Oct': 'October',\n","      'Nov': 'November',\n","      'Dec': 'December'\n","  }\n","\n","  # For 'MonthYear.y'\n","  df['MonthYear.y'] = df['MonthYear.y'].str.replace(r'^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)', lambda x: month_map[x.group(0)], regex=True)\n","  df['MonthYear.y'] = pd.to_datetime(df['MonthYear.y'], format='%B %Y', errors='coerce')\n","  df['MonthYear.y'] = df['MonthYear.y'].fillna(pd.Timestamp(0))  # Set invalid dates to 0\n","\n","  # Date.of.Birth\n","  # Convert Date.of.Birth to datetime, setting errors='coerce' to handle invalid formats\n","  df['Date.of.Birth'] = pd.to_datetime(df['Date.of.Birth'], format='%m/%d/%Y', errors='coerce')\n","\n","  # Replace NaT (invalid dates) with 0\n","  df['Date.of.Birth'] = df['Date.of.Birth'].fillna(pd.Timestamp(0))\n","\n","\n","  # Outcome.Type\n","  le = LabelEncoder()\n","  df['Outcome.Type'] = le.fit_transform(df['Outcome.Type'].astype(str))\n","\n","  # Outcome.Subtype\n","  le = LabelEncoder()\n","  df['Outcome.Subtype'] = le.fit_transform(df['Outcome.Subtype'].astype(str))\n","\n","  # Animal.Type.y\n","  le = LabelEncoder()\n","  df['Animal.Type.y'] = le.fit_transform(df['Animal.Type.y'].astype(str))\n","\n","  # Sex.upon.Outcome\n","  le = LabelEncoder()\n","  df['Sex.upon.Outcome'] = le.fit_transform(df['Sex.upon.Outcome'].astype(str))\n","\n","  # Age.upon.Outcome\n","  df['Age.upon.Outcome'] = df['Age.upon.Outcome'].str.split(' ').str[0].fillna('0').astype(int)\n","\n","  # Breed.y\n","  le = LabelEncoder()\n","  df['Breed.y'] = le.fit_transform(df['Breed.y'].astype(str))\n","\n","  # Color.y\n","  le = LabelEncoder()\n","  df['Color.y'] = le.fit_transform(df['Color.y'].astype(str))\n","\n","  # DateTimeUTC.y\n","  df['DateTimeUTC.y'] = pd.to_datetime(df['DateTimeUTC.y'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","\n","  # Convert specified datetime columns and handle invalid entries by replacing with 0\n","  datetime_columns = [\n","      'DateTime.x', 'MonthYear.x', 'DateTimeUTC.x',\n","      'DateTime.y', 'MonthYear.y', 'Date.of.Birth', 'DateTimeUTC.y'\n","  ]\n","\n","  for col in datetime_columns:\n","      df[col] = pd.to_datetime(df[col], errors='coerce')  # Convert to datetime, invalids become NaT\n","      df[col].fillna(pd.Timestamp(0), inplace=True)  # Replace NaT with 0\n","\n","  df.drop(columns=datetime_columns, inplace=True)\n","\n","\n","\n","\n","  return df\n","\n","austin_data = austin_preprocess(austin_data)\n","\n","# imputer\n","def impute_missing_values(df):\n","    for column in df.columns:\n","        if df[column].dtype in ['float64', 'int64']:\n","            df[column].fillna(df[column].mean(), inplace=True)  # Impute with mean\n","        else:\n","            df[column].fillna(df[column].mode()[0], inplace=True)  # Impute with mode\n","    return df\n","\n","austin_data = impute_missing_values(austin_data)\n","# print(\"Null Checks\")\n","# print(indiana_data.isnull().sum())\n","# print(\"Infinity Checks\")\n","# print(np.isinf(indiana_data).sum())\n","\n","\n","# austin_data.to_csv('austin_processed.csv', index=False)\n","\n","x_train = austin_data.iloc[:10000].drop(columns=['Outcome.Type'])\n","y_train = austin_data.iloc[:10000]['Outcome.Type']\n","\n","# model training and featured score\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","#print(x_train.dtypes)\n","\n","\n","# Build the Random Forest model\n","rf_model = RandomForestClassifier()\n","rf_model.fit(x_train, y_train)\n","\n","# Get feature importances\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","feature_importances = rf_model.feature_importances_\n","feature_names = x_train.columns\n","\n","# Create a bar chart\n","plt.figure(figsize=(10, 6))\n","indices = np.argsort(feature_importances)[::-1]  # Sort feature importances in descending order\n","plt.bar(range(len(feature_importances)), feature_importances[indices], align='center')\n","plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n","plt.title('Feature Austin Importances from Random Forest')\n","plt.xlabel('Features')\n","plt.ylabel('Importance Score')\n","plt.tight_layout()\n","plt.show()\n","\n","# print(indiana_data['breedname'].value_counts())\n","# print(indiana_data['movementtype'].value_counts())\n","\n","\n","# Calculate the correlation score for Breed and Outcome.Type\n","correlation_Breed_x = austin_data['Breed.x'].astype('category').cat.codes.corr(austin_data['Outcome.Type'].astype('category').cat.codes)\n","correlation_Breed_y = austin_data['Breed.y'].astype('category').cat.codes.corr(austin_data['Outcome.Type'].astype('category').cat.codes)\n","\n","\n","# Print the correlation score\n","print(\"Correlation score between Breed.x and Outcome.Type:\", correlation_Breed_x)\n","print(\"Correlation score between Breed.y and Outcome.Type:\", correlation_Breed_y)\n","\n"],"metadata":{"id":"PQalqhOQQlQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 25/9/2024 12.01 pm optipaw_new Preproceesing Code for ML\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","\n","optipaw_data = pd.read_csv('optipaw_new.csv')\n","\n","# Preprocessing Function\n","\n","# DROP ANIMAL ID\n","\n","def optipaw_preprocessing(df):\n","\n","  # For Animal.ID, strip the A if it starts with A and convert it to int, else convert to int\n","  df['Animal.ID'] = df['Animal.ID'].apply(lambda x: int(x[1:]) if isinstance(x, str) and x.startswith('A') else int(x))\n","\n","  # For Name, we will use label encoding to assign each unique name a specific int\n","  label_encoder = LabelEncoder()\n","  df['Name'] = label_encoder.fit_transform(df['Name'].astype(str))\n","\n","  # For Animal.Type we will be using label encoding again\n","  label_encoder = LabelEncoder()\n","  df['Animal.Type'] = label_encoder.fit_transform(df['Animal.Type'].astype(str))\n","\n","  # For Sex, we will using label encoding too\n","  label_encoder = LabelEncoder()\n","  df['Sex'] = label_encoder.fit_transform(df['Sex'].astype(str))\n","\n","  # For colours, we will split into individual colours and use one hot encoding, which is assigning binary values to it\n","\n","  # Split the 'Color' column by '/', 'and', and ','\n","  df['Color'] = df['Color'].str.replace('/', ' ').str.replace('and', ' ').str.replace(',', ' ')\n","\n","  # Split the 'Color' column into a list and capitalize the first letter of each word\n","  df['Color'] = df['Color'].str.split().apply(lambda colors: [color.capitalize() for color in colors])\n","\n","  # Create dummy variables for each unique color\n","  df_colors = df['Color'].str.join(' ').str.get_dummies(sep=' ')\n","\n","  # Concatenate the original dataframe with the one-hot encoded color dataframe\n","  df = pd.concat([df, df_colors], axis=1)\n","\n","  # Drop the original 'Color' column\n","  df = df.drop(columns=['Color'])\n","\n","  # For Age, we will just store it as int and impute it with 0 if it is null, and store it as float\n","  df['Age'] = df['Age'].fillna(0).astype(float)  # Keep as float to handle decimals\n","\n","  # For Intake.Type, we will just use label encoding\n","  label_encoder = LabelEncoder()\n","  df['Intake.Type'] = label_encoder.fit_transform(df['Intake.Type'].astype(str))\n","\n","  # Same goes to Outcome.Type\n","  label_encoder = LabelEncoder()\n","  df['Outcome.Type'] = label_encoder.fit_transform(df['Outcome.Type'].astype(str))\n","\n","  # For Date and Time, we will be using panda and numpy date conversion\n","\n","  # Convert Intake.Date and Outcome.Date to datetime format\n","  df['Intake.Date'] = pd.to_datetime(df['Intake.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","  df['Outcome.Date'] = pd.to_datetime(df['Outcome.Date'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n","\n","\n","  # Extract date components from the date columns\n","  df['Intake.Day'] = df['Intake.Date'].dt.day.fillna(0).astype(int)\n","  df['Intake.Month'] = df['Intake.Date'].dt.month.fillna(0).astype(int)\n","  df['Intake.Year'] = df['Intake.Date'].dt.year.fillna(0).astype(int)\n","\n","  df['Outcome.Day'] = df['Outcome.Date'].dt.day.fillna(0).astype(int)\n","  df['Outcome.Month'] = df['Outcome.Date'].dt.month.fillna(0).astype(int)\n","  df['Outcome.Year'] = df['Outcome.Date'].dt.year.fillna(0).astype(int)\n","\n","  # Extract and convert the hour to radians\n","  df['Intake.Hour'] = df['Intake.Date'].dt.hour.fillna(0).astype(int)\n","  df['Outcome.Hour'] = df['Outcome.Date'].dt.hour.fillna(0).astype(int)\n","\n","  df['Intake.Hour.Radians'] = (df['Intake.Hour'] / 24) * 2 * np.pi\n","  df['Outcome.Hour.Radians'] = (df['Outcome.Hour'] / 24) * 2 * np.pi\n","\n","  # Drop original date columns if no longer needed\n","  df = df.drop(columns=['Intake.Date', 'Outcome.Date'])\n","\n","  return df\n","\n","optipaw_data = optipaw_preprocessing(optipaw_data)\n","\n","train_data = optipaw_data[optipaw_data['State'] == 'Austin'].copy()\n","test_data = optipaw_data[optipaw_data['State'] != 'Austin'].copy()\n","\n","# Drop 'State' column from both train and test datasets\n","train_data = train_data.drop(columns=['State'])\n","test_data = test_data.drop(columns=['State'])\n","\n","print(train_data.head(5))\n","print(test_data.head(5))\n","\n","\n","# Set options to display all unique values\n","pd.set_option('display.max_rows', None)  # Display all rows\n","pd.set_option('display.max_columns', None)  # Display all columns\n","\n","# Check for datatype of each column\n","print(train_data.dtypes)\n","print(test_data.dtypes)\n","\n","print(train_data.shape)\n","print(test_data.shape) I'll probably create another colour this is pretty messy yeah oh yeah\n","\n","# Calculate unique values and missing values for each column\n","unique_values = train_data.nunique()\n","unique_values2 = test_data.nunique()\n","\n","\n","# missing_values = test_data.isnull().sum()\n","# missing_values2 = train_data.isnull().sum()\n","\n","print(\"Train data unique:\")\n","print(unique_values)\n","print(\"Test data unique:\")\n","print(unique_values2)\n","# print(missing_values)\n","# print(missing_values2)\n","\n","train_data.head(1000).to_csv('train.csv', index=False)\n","test_data.head(1000).to_csv('test.csv', index=False)\n","\n","\n","# Will delete soon, this column is to check featured scores, and to also check whether ml model takes in the processed data\n","\n","x_train = train_data.iloc[:10000].drop(columns=['Outcome.Type'])\n","y_train = train_data.iloc[:10000]['Outcome.Type']\n","\n","x_test = test_data.iloc[:10000].drop(columns=['Outcome.Type'])\n","y_test = test_data.iloc[:10000]['Outcome.Type']\n","\n","\n","# model training and featured score\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","\n","#print(x_train.dtypes)\n","\n","\n","# Build the Random Forest model\n","rftrain_model = RandomForestClassifier()\n","rftrain_model.fit(x_train, y_train)\n","\n","rftest_model = RandomForestClassifier()\n","rftest_model.fit(x_test, y_test)\n","\n","\n","# Feature importances Bar Chart function\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_feature_importances(rf_model, x_train, title):\n","    \"\"\"\n","    Plots the feature importances from a Random Forest model.\n","\n","    Parameters:\n","    rf_model: Trained Random Forest model\n","    x_train: DataFrame containing feature names\n","    title: Title to be included in the plot\n","    \"\"\"\n","    feature_importances = rf_model.feature_importances_\n","    feature_names = x_train.columns\n","\n","    # Create a bar chart\n","    plt.figure(figsize=(10, 6))\n","    indices = np.argsort(feature_importances)[::-1]  # Sort feature importances in descending order\n","    plt.bar(range(len(feature_importances)), feature_importances[indices], align='center')\n","    plt.xticks(range(len(feature_importances)), feature_names[indices], rotation=90)\n","    plt.title(f'Feature Importances from Random Forest - {title} Dataset')\n","    plt.xlabel('Features')\n","    plt.ylabel('Importance Score')\n","    plt.tight_layout()\n","    plt.show()\n","\n","plot_feature_importances(rftrain_model, x_train, 'Train')\n","plot_feature_importances(rftest_model, x_test, 'Test')\n","\n","# Building function for later (scoring)\n","from sklearn.metrics import accuracy_score\n","\n","# Build this function\n","def calculate_accuracy(y_test, y_pred):\n","    \"\"\"\n","    Calculate the accuracy score given the true labels and predicted labels.\n","\n","    Parameters:\n","    - y_test: array-like, true labels\n","    - y_pred: array-like, predicted labels\n","\n","    Returns:\n","    - accuracy: float, the accuracy score\n","    \"\"\"\n","    accuracy = accuracy_score(y_test, y_pred)\n","    return accuracy\n","\n"],"metadata":{"id":"rKzRza4uy8nR"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}